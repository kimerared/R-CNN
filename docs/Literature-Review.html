<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computer Vision: Object Detection using Faster R-CNN, Mask R-CNN, and YOLOv8 - 2&nbsp; Literature Review</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./methodology.html" rel="next">
<link href="./intro.html" rel="prev">
<link href="./book-favicon-512.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Literature-Review.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature Review</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer Vision: Object Detection using Faster R-CNN, Mask R-CNN, and YOLOv8</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/kimerared/R-CNN.git" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Computer-Vision--Object-Detection-using-Faster-R-CNN,-Mask-R-CNN,-and-YOLOv8.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cover.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">RCNN</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Literature-Review.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./methodology.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Methodologies</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Results</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./yolov9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">YOLOv9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Contributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#r-cnn-region-based-convolutional-network" id="toc-r-cnn-region-based-convolutional-network" class="nav-link active" data-scroll-target="#r-cnn-region-based-convolutional-network"><span class="header-section-number">2.1</span> R-CNN (Region-based Convolutional Network)</a>
  <ul class="collapse">
  <li><a href="#region-proposal-generation" id="toc-region-proposal-generation" class="nav-link" data-scroll-target="#region-proposal-generation"><span class="header-section-number">2.1.1</span> Region Proposal Generation</a></li>
  <li><a href="#feature-extraction" id="toc-feature-extraction" class="nav-link" data-scroll-target="#feature-extraction"><span class="header-section-number">2.1.2</span> Feature Extraction</a></li>
  <li><a href="#selective-search" id="toc-selective-search" class="nav-link" data-scroll-target="#selective-search"><span class="header-section-number">2.1.3</span> Selective Search</a></li>
  <li><a href="#fine-tuning-and-classification" id="toc-fine-tuning-and-classification" class="nav-link" data-scroll-target="#fine-tuning-and-classification"><span class="header-section-number">2.1.4</span> Fine-tuning and Classification</a></li>
  <li><a href="#bounding-box-regression" id="toc-bounding-box-regression" class="nav-link" data-scroll-target="#bounding-box-regression"><span class="header-section-number">2.1.5</span> Bounding Box Regression</a></li>
  <li><a href="#non-maximum-suppression" id="toc-non-maximum-suppression" class="nav-link" data-scroll-target="#non-maximum-suppression"><span class="header-section-number">2.1.6</span> Non-Maximum Suppression</a></li>
  <li><a href="#drawbacks" id="toc-drawbacks" class="nav-link" data-scroll-target="#drawbacks"><span class="header-section-number">2.1.7</span> Drawbacks</a></li>
  </ul></li>
  <li><a href="#fast-r-cnn" id="toc-fast-r-cnn" class="nav-link" data-scroll-target="#fast-r-cnn"><span class="header-section-number">2.2</span> Fast R-CNN</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture"><span class="header-section-number">2.2.1</span> Architecture</a></li>
  <li><a href="#roi-pooling-layer" id="toc-roi-pooling-layer" class="nav-link" data-scroll-target="#roi-pooling-layer"><span class="header-section-number">2.2.2</span> RoI Pooling Layer</a></li>
  <li><a href="#advantages-of-fast-r-cnn" id="toc-advantages-of-fast-r-cnn" class="nav-link" data-scroll-target="#advantages-of-fast-r-cnn"><span class="header-section-number">2.2.3</span> Advantages of Fast R-CNN</a></li>
  </ul></li>
  <li><a href="#faster-r-cnn" id="toc-faster-r-cnn" class="nav-link" data-scroll-target="#faster-r-cnn"><span class="header-section-number">2.3</span> Faster R-CNN</a>
  <ul class="collapse">
  <li><a href="#region-proposal-network-rpn" id="toc-region-proposal-network-rpn" class="nav-link" data-scroll-target="#region-proposal-network-rpn"><span class="header-section-number">2.3.1</span> Region Proposal Network (RPN)</a></li>
  <li><a href="#anchor-boxes" id="toc-anchor-boxes" class="nav-link" data-scroll-target="#anchor-boxes"><span class="header-section-number">2.3.2</span> Anchor Boxes</a></li>
  <li><a href="#region-of-interest-roi" id="toc-region-of-interest-roi" class="nav-link" data-scroll-target="#region-of-interest-roi"><span class="header-section-number">2.3.3</span> Region of Interest (RoI)</a></li>
  <li><a href="#feature-pyramid-network-fpn" id="toc-feature-pyramid-network-fpn" class="nav-link" data-scroll-target="#feature-pyramid-network-fpn"><span class="header-section-number">2.3.4</span> Feature Pyramid Network (FPN)</a></li>
  <li><a href="#classifier-and-bounding-box-regressor" id="toc-classifier-and-bounding-box-regressor" class="nav-link" data-scroll-target="#classifier-and-bounding-box-regressor"><span class="header-section-number">2.3.5</span> Classifier and Bounding Box Regressor</a></li>
  <li><a href="#non-maximum-suppression-nms" id="toc-non-maximum-suppression-nms" class="nav-link" data-scroll-target="#non-maximum-suppression-nms"><span class="header-section-number">2.3.6</span> Non-Maximum Suppression (NMS)</a></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output"><span class="header-section-number">2.3.7</span> Output</a></li>
  <li><a href="#advantages-of-faster-r-cnn" id="toc-advantages-of-faster-r-cnn" class="nav-link" data-scroll-target="#advantages-of-faster-r-cnn"><span class="header-section-number">2.3.8</span> Advantages of Faster R-CNN</a></li>
  </ul></li>
  <li><a href="#mask-r-cnn" id="toc-mask-r-cnn" class="nav-link" data-scroll-target="#mask-r-cnn"><span class="header-section-number">2.4</span> Mask R-CNN</a>
  <ul class="collapse">
  <li><a href="#backbone-cnn" id="toc-backbone-cnn" class="nav-link" data-scroll-target="#backbone-cnn"><span class="header-section-number">2.4.1</span> Backbone CNN</a></li>
  <li><a href="#region-proposal-network-rpn-1" id="toc-region-proposal-network-rpn-1" class="nav-link" data-scroll-target="#region-proposal-network-rpn-1"><span class="header-section-number">2.4.2</span> Region Proposal Network (RPN)</a></li>
  <li><a href="#region-of-interest-align-roialign" id="toc-region-of-interest-align-roialign" class="nav-link" data-scroll-target="#region-of-interest-align-roialign"><span class="header-section-number">2.4.3</span> Region of interest Align (RoIAlign)</a></li>
  <li><a href="#parallel-branches" id="toc-parallel-branches" class="nav-link" data-scroll-target="#parallel-branches"><span class="header-section-number">2.4.4</span> Parallel Branches</a></li>
  <li><a href="#advantages-over-faster-r-cnn" id="toc-advantages-over-faster-r-cnn" class="nav-link" data-scroll-target="#advantages-over-faster-r-cnn"><span class="header-section-number">2.4.5</span> Advantages over Faster R-CNN</a></li>
  </ul></li>
  <li><a href="#yolo-you-only-look-once" id="toc-yolo-you-only-look-once" class="nav-link" data-scroll-target="#yolo-you-only-look-once"><span class="header-section-number">2.5</span> YOLO (You Only Look Once)</a>
  <ul class="collapse">
  <li><a href="#architecture-1" id="toc-architecture-1" class="nav-link" data-scroll-target="#architecture-1"><span class="header-section-number">2.5.1</span> Architecture</a></li>
  <li><a href="#anchor-free-approach" id="toc-anchor-free-approach" class="nav-link" data-scroll-target="#anchor-free-approach"><span class="header-section-number">2.5.2</span> Anchor-Free Approach</a></li>
  <li><a href="#efficient-backbone" id="toc-efficient-backbone" class="nav-link" data-scroll-target="#efficient-backbone"><span class="header-section-number">2.5.3</span> Efficient Backbone</a></li>
  <li><a href="#improved-object-detection" id="toc-improved-object-detection" class="nav-link" data-scroll-target="#improved-object-detection"><span class="header-section-number">2.5.4</span> Improved Object Detection</a></li>
  <li><a href="#real-time-performance" id="toc-real-time-performance" class="nav-link" data-scroll-target="#real-time-performance"><span class="header-section-number">2.5.5</span> Real-Time Performance</a></li>
  <li><a href="#advantages-over-mask-r-cnn" id="toc-advantages-over-mask-r-cnn" class="nav-link" data-scroll-target="#advantages-over-mask-r-cnn"><span class="header-section-number">2.5.6</span> Advantages over Mask R-CNN</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature Review</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The evolution from R-CNN to faster R-CNN represents a significant advancement in object detection algorithms, especially in speed and efficiency. A brief history of the development progression of R-CNN, Fast R-CNN, and Faster R-CNN adds valuable context to our study on object detection architectures. It will help to understand the evolution of these models, the motivations behind their development, and the improvements made over time.</p>
<section id="r-cnn-region-based-convolutional-network" class="level2 page-columns page-full" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="r-cnn-region-based-convolutional-network"><span class="header-section-number">2.1</span> R-CNN (Region-based Convolutional Network)</h2>
<p>R-CNN was a breakthrough in object detection. It employed a multi-stage approach that involved a selective search for generating region proposals followed by a convolutional neural network for feature extraction and a support vector machine (SVM) for object classification within each region.</p>
<section id="region-proposal-generation" class="level3 page-columns page-full" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="region-proposal-generation"><span class="header-section-number">2.1.1</span> Region Proposal Generation</h3>
<p>R-CNN began with generating region proposals using the selective search algorithm <span class="citation" data-cites="girshick2014rich"><a href="references.html#ref-girshick2014rich" role="doc-biblioref">[1]</a></span>. Selective search is a method for identifying potential object regions in an image based on low-level features such as color, texture, and intensity. It produces a set of bounding boxes that mostly have objects.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="feature-extraction" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="feature-extraction"><span class="header-section-number">2.1.2</span> Feature Extraction</h3>
<p>Following the generation of region proposals, R-CNN employed a pre-trained convolutional neural network to independently extract features from each region. Typically, the chosen CNN model was AlexNet, pre-trained on the ImageNet dataset specifically for image classification tasks.</p>
</section>
<section id="selective-search" class="level3 page-columns page-full" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="selective-search"><span class="header-section-number">2.1.3</span> Selective Search</h3>
<p>Selective search employs a Hierarchical Grouping algorithm to detect objects with diverse characteristics such as scales, colors, textures, and enclosures. Initially, the method utilizes the Felzenszwalb and Huttenlocher <span class="citation" data-cites="felzenszwalbmethod"><a href="references.html#ref-felzenszwalbmethod" role="doc-biblioref">[2]</a></span> approach to segment the image into regions and obtain a set of initial regions. The grouping algorithm is detailed in the provided Algorithm.</p>
<div class="no-row-height column-margin column-container"><div id="ref-felzenszwalbmethod" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">Author(s), <span>“Segmentation of images into regions using felzenszwalb and huttenlocher method,”</span> <em>Journal/Conference</em>, vol. Volume, no. Number, p. Pages, Publication Year.</div>
</div></div></section>
<section id="fine-tuning-and-classification" class="level3 page-columns page-full" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="fine-tuning-and-classification"><span class="header-section-number">2.1.4</span> Fine-tuning and Classification</h3>
<p>Following feature extraction, the extracted features were input into a distinct classifier to ascertain the presence of objects within the regions. R-CNN utilized a support vector machine (SVM) <span class="citation" data-cites="girshick2014rich"><a href="references.html#ref-girshick2014rich" role="doc-biblioref">[1]</a></span> for this classification task. Each SVM was trained to discern whether the feature corresponded to a particular object or background.</p>
<div class="no-row-height column-margin column-container"><div id="ref-girshick2014rich" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">R. Girshick, J. Donahue, T. Darrell, and J. Malik, <span>“Rich feature hierarchies for accurate object detection and semantic segmentation,”</span> in <em>2014 IEEE conference on computer vision and pattern recognition</em>, 2014, pp. 580–587. doi: <a href="https://doi.org/10.1109/CVPR.2014.81">10.1109/CVPR.2014.81</a></div>
</div></div></section>
<section id="bounding-box-regression" class="level3" data-number="2.1.5">
<h3 data-number="2.1.5" class="anchored" data-anchor-id="bounding-box-regression"><span class="header-section-number">2.1.5</span> Bounding Box Regression</h3>
<p>Following classification, R-CNN conducted bounding box regression to enhance the accuracy of the detected object locations. This process adjusts the bounding boxes produced by the region proposal algorithm to align more precisely with the actual object locations within the regions.</p>
</section>
<section id="non-maximum-suppression" class="level3" data-number="2.1.6">
<h3 data-number="2.1.6" class="anchored" data-anchor-id="non-maximum-suppression"><span class="header-section-number">2.1.6</span> Non-Maximum Suppression</h3>
<p>Lastly, R-CNN implemented non-maximum suppression to eliminate redundant detections, ensuring that each object is detected only once.</p>
</section>
<section id="drawbacks" class="level3" data-number="2.1.7">
<h3 data-number="2.1.7" class="anchored" data-anchor-id="drawbacks"><span class="header-section-number">2.1.7</span> Drawbacks</h3>
<p>An initial drawback of the R-CNN architecture was its computational inefficiency during inference, primarily stemming from its sequential processing of region proposals. Processing each region proposal independently led to redundant computations and prolonged inference times.</p>
<div class="quarto-figure quarto-figure-center" style="text-align:center;">
<figure class="figure">
<p><img src="images/flowcharts/RCNN-flowcharts.jpg" class="img-fluid figure-img" alt="R-CNN Flowchart"></p>
<figcaption class="figure-caption">Flowchart of R-CNN</figcaption>
</figure>
</div>
<div style="page-break-after: always;"></div>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="fast-r-cnn" class="level2 page-columns page-full" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="fast-r-cnn"><span class="header-section-number">2.2</span> Fast R-CNN</h2>
<p>Fast R-CNN, a significant advancement in object detection, introduces several innovations enhancing both training and testing efficiency while improving detection accuracy. Unlike its predecessors, Fast R-CNN leverages the VGG16 network, achieving a remarkable 9x increase in training speed compared to R-CNN <span class="citation" data-cites="girshick2015fastrcnn"><a href="references.html#ref-girshick2015fastrcnn" role="doc-biblioref">[3]</a></span>. At test-time, Fast R-CNN demonstrates an impressive speed improvement of 213x, making it significantly faster, and more practical for real-world applications. Additionally, Fast R-CNN outperforms previous methods in terms of mean Average Precision (mAP) on benchmark datasets like PASCAL VOC 2012 <span class="citation" data-cites="girshick2015fastrcnn"><a href="references.html#ref-girshick2015fastrcnn" role="doc-biblioref">[3]</a></span>.</p>
<div class="no-row-height column-margin column-container"></div><p>Fast R-CNN tackles the computational inefficiencies of R-CNN by introducing a unified architecture that consolidates region proposal generation, feature extraction, and object classification into a single network. This approach dramatically reduces redundant computations, and accelerates the inference process.</p>
<section id="architecture" class="level3 page-columns page-full" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="architecture"><span class="header-section-number">2.2.1</span> Architecture</h3>
<p>Expanding upon its architecture, Fast R-CNN incorporates several key components to achieve its performance gains. Fast R-CNN addresses the speed and efficiency limitations of R-CNN by proposing a unified architecture that integrates region proposal generation, feature extraction, and object classification into a single network.</p>
<p>Firstly, it utilizes a Region Proposal Network (RPN) to generate region proposals directly from the convolutional feature maps, eliminating the need for external proposal methods like selective search. The RPN functions by sliding a small grid (essentially a compact CNN) across the evolving feature map to predict the spatial dimensions (bounding boxes) and associated probability scores for objects within each sliding window. This streamlines the detection process and enhances efficiency. Furthermore, Fast R-CNN introduces the Region of Interest (RoI) pooling layer <span class="citation" data-cites="girshick2015fastrcnn"><a href="references.html#ref-girshick2015fastrcnn" role="doc-biblioref">[3]</a></span>, allowing feature extraction from region proposals of varying sizes without the need for expensive resizing operations. This enables precise alignment of features with the corresponding regions of interest, leading to improved localization accuracy. Moreover, Fast R-CNN adopts a unified network architecture, enabling end-to-end training of both the region proposal and object detection tasks. This approach ensures better optimization, and facilitates seamless integration of different components, contributing to the overall efficiency and effectiveness of the model.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="roi-pooling-layer" class="level3 page-columns page-full" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="roi-pooling-layer"><span class="header-section-number">2.2.2</span> RoI Pooling Layer</h3>
<p>The RoI pooling layer employs max pooling to transform features within any valid region of interest into a compact feature map with a fixed spatial extent of H × W (e.g., 7 × 7) <span class="citation" data-cites="girshick2015fastrcnn"><a href="references.html#ref-girshick2015fastrcnn" role="doc-biblioref">[3]</a></span>, where H and W are layer hyper-parameters independent of any specific RoI. A more efficient training strategy leverages feature sharing throughout the process. During Fast R-CNN training, stochastic gradient descent (SGD) minibatches are hierarchically sampled: N images are initially sampled, followed by R/N RoIs from each image. Notably, both during forward and backward passes, ROIs from the same image share computation and memory. Fixed-size feature maps yielded from RoI pooling are input to fully connected layers for object classification and bounding-box regression. Feature classification aims to detect features and assign class probabilities for each proposed location, while bounding box regression seeks to enhance the localization accuracy of detected features. Fast R-CNN introduces multitasking loss functions that amalgamate classification and bounding box regression losses. This enables joint training of both tasks, ensuring the network learns to predict object location and precise bounding boxes concurrently.</p>
<div class="no-row-height column-margin column-container"><div id="ref-girshick2015fastrcnn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">R. Girshick, <span>“Fast r-CNN,”</span> in <em>2015 IEEE international conference on computer vision (ICCV)</em>, 2015, pp. 1440–1448. doi: <a href="https://doi.org/10.1109/ICCV.2015.169">10.1109/ICCV.2015.169</a></div>
</div></div><div class="quarto-figure quarto-figure-center" style="text-align: center;">
<figure class="figure">
<p><img src="images/flowcharts/fast-RCNN-flowcharts.jpg" class="img-fluid figure-img" alt="Fast R-CNN Flowchart"></p>
<figcaption class="figure-caption">Flowchart of Fast R-CNN</figcaption>
</figure>
</div>
</section>
<section id="advantages-of-fast-r-cnn" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="advantages-of-fast-r-cnn"><span class="header-section-number">2.2.3</span> Advantages of Fast R-CNN</h3>
<p>In contrast to R-CNN, which comprises multiple independent steps (such as region proposal generation, feature extraction, classification, and bounding box regression), Fast R-CNN integrates these processes within a unified network architecture. This enables end-to-end training of the entire pipeline, leading to improved optimization and potentially higher accuracy. Fast R-CNN achieves variable sharing across all spatial dimensions of an image. Unlike R-CNN, which extracts features independently for each image, Fast R-CNN performs feature extraction on the entire image only once. This shared computation across convolutional features significantly minimizes redundant calculations, expediting the inference process.</p>
</section>
</section>
<section id="faster-r-cnn" class="level2 page-columns page-full" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="faster-r-cnn"><span class="header-section-number">2.3</span> Faster R-CNN</h2>
<p>Compared to preceding methodologies, Faster R-CNN achieves more efficient object recognition by seamlessly integrating regional proposal steps directly into the network architecture. Subsequent advancements have expanded upon this framework, establishing it as a fundamental component in object recognition. The architecture commences with a feature extraction backbone, typically a convolutional neural network (CNN) pre-trained on extensive datasets such as ImageNet. This backbone extracts pertinent features from the input image.</p>
<section id="region-proposal-network-rpn" class="level3 page-columns page-full" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="region-proposal-network-rpn"><span class="header-section-number">2.3.1</span> Region Proposal Network (RPN)</h3>
<p>Faster R-CNN further improves the efficiency of object detection by introducing the Region Proposal Network (RPN), which generates region proposals directly from the convolutional feature maps. It eliminates the need for external region proposal methods like Selective Search, resulting in faster and more accurate proposal generation.</p>
<p>The feature maps derived from the backbone network are input to a Region Proposal Network (RPN) <span class="citation" data-cites="ren2015fasterrcnn"><a href="references.html#ref-ren2015fasterrcnn" role="doc-biblioref">[4]</a></span>. The RPN, a compact fully convolutional network, employs a sliding window approach (typically 3x3) over the feature maps to generate region proposals. The RPN yields a collection of bounding box proposals accompanied by objectness scores, indicating the probability of containing an object. These proposals are generated by leveraging predefined anchor boxes of varying scales and aspect ratios.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="anchor-boxes" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="anchor-boxes"><span class="header-section-number">2.3.2</span> Anchor Boxes</h3>
<p>The RPN generates region proposals by predicting offsets and scales for a predefined set of anchor boxes at each spatial position in the feature maps. These anchor boxes, varying in size and aspect ratio, serve as reference boxes for the proposal generation process.</p>
</section>
<section id="region-of-interest-roi" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="region-of-interest-roi"><span class="header-section-number">2.3.3</span> Region of Interest (RoI)</h3>
<p>Region of Interest (RoI) refers to a specific area or region within an image that is selected for further analysis or processing. In the context of object detection and image segmentation tasks, RoIs typically represent regions where objects of interest are located.</p>
</section>
<section id="feature-pyramid-network-fpn" class="level3 page-columns page-full" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="feature-pyramid-network-fpn"><span class="header-section-number">2.3.4</span> Feature Pyramid Network (FPN)</h3>
<p>Feature Pyramid Network (FPN) stands as a pivotal advancement in object detection methodologies, offering a robust solution for handling objects of varying scales and complexities. FPN, introduced by Facebook AI Research (FAIR), Cornell University, and Cornell Tech, integrates a pyramidal feature hierarchy that combines low-resolution, semantically strong features with high-resolution, semantically weak features through top-down pathways and lateral connections.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full" style="text-align: center;">
<figure class="figure page-columns page-full">
<p><img src="images/architecture/pyramid-network-diagram.png" class="img-fluid figure-img" alt="Feature Pyramid Network (FPN)"></p>
<figcaption class="figure-caption">Feature Pyramid Network (FPN) <span class="citation" data-cites="lin2016feature"><a href="references.html#ref-lin2016feature" role="doc-biblioref">[5]</a></span></figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-lin2016feature" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">T.-Y. Lin, P. Dollár, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie, <span>“Feature pyramid networks for object detection,”</span> <em>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 936–944, 2016, Available: <a href="https://api.semanticscholar.org/CorpusID:10716717">https://api.semanticscholar.org/CorpusID:10716717</a></div>
</div></div></figure>
</div>
<p>This innovative approach ensures rich semantic information at all levels without compromising speed or memory efficiency. The FPN architecture has demonstrated superior performance in region proposal networks (RPN) and detection networks, showcasing remarkable improvements in detecting small objects and enhancing robustness to scale variations.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full" style="text-align:center;">
<figure class="figure page-columns page-full">
<p><img src="images/architecture/RPN-diagram.png" class="img-fluid figure-img" alt="RPN diagram"></p>
<figcaption class="figure-caption">Faster R-CNN different stages <span class="citation" data-cites="ren2015fasterrcnn"><a href="references.html#ref-ren2015fasterrcnn" role="doc-biblioref">[4]</a></span></figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
</section>
<section id="classifier-and-bounding-box-regressor" class="level3" data-number="2.3.5">
<h3 data-number="2.3.5" class="anchored" data-anchor-id="classifier-and-bounding-box-regressor"><span class="header-section-number">2.3.5</span> Classifier and Bounding Box Regressor</h3>
<p>The RoI-pooled features are separately input into branches for classification and bounding box regression.</p>
<ul>
<li><strong>Classification:</strong> The features traverse a classifier (e.g., fully connected layers followed by softmax) to predict the probability of each region proposal belonging to different object classes. Bounding Box Regression: Another set of fully connected layers predicts refined bounding box coordinates for each region proposal.</li>
<li><strong>Loss Function:</strong> The network undergoes end-to-end training using a multi-task loss function, which combines losses from the RPN (objectness score prediction and bounding box regression) with those from the classification and bounding box regression branches.</li>
</ul>
</section>
<section id="non-maximum-suppression-nms" class="level3" data-number="2.3.6">
<h3 data-number="2.3.6" class="anchored" data-anchor-id="non-maximum-suppression-nms"><span class="header-section-number">2.3.6</span> Non-Maximum Suppression (NMS)</h3>
<p>Following prediction, non-maximum suppression is employed to discard redundant and overlapping detections based on their confidence scores and bounding box coordinates.</p>
</section>
<section id="output" class="level3 page-columns page-full" data-number="2.3.7">
<h3 data-number="2.3.7" class="anchored" data-anchor-id="output"><span class="header-section-number">2.3.7</span> Output</h3>
<p>The final output comprises a collection of object detections alongside their bounding boxes and corresponding class labels.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full" style="text-align:center;">
<figure class="figure page-columns page-full">
<p><img src="images/flowcharts/faster-RCNN-flowcharts.jpg" class="img-fluid figure-img" alt="Faster R-CNN Flowchart"></p>
<figcaption class="figure-caption">Flowchart of Faster R-CNN <span class="citation" data-cites="ren2015fasterrcnn"><a href="references.html#ref-ren2015fasterrcnn" role="doc-biblioref">[4]</a></span></figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
</section>
<section id="advantages-of-faster-r-cnn" class="level3 page-columns page-full" data-number="2.3.8">
<h3 data-number="2.3.8" class="anchored" data-anchor-id="advantages-of-faster-r-cnn"><span class="header-section-number">2.3.8</span> Advantages of Faster R-CNN</h3>
<ul>
<li><strong>Efficient Region Proposal Generation:</strong> The Region Proposal Network (RPN) significantly reduces the computational load of region proposal methods, enabling nearly cost-free region proposals by sharing convolutional features.</li>
<li><strong>End-to-End Training:</strong> RPN and Fast R-CNN can be jointly trained to share convolutional features, leading to a unified network architecture.</li>
<li><strong>High-Quality Proposals:</strong> RPN produces high-quality region proposals, enhancing detection accuracy compared to traditional methods like Selective Search and EdgeBoxes.</li>
<li><strong>Practical Implementation:</strong> The proposed method achieves state-of-the-art object detection accuracy on benchmarks like PASCAL VOC <span class="citation" data-cites="ren2015fasterrcnn"><a href="references.html#ref-ren2015fasterrcnn" role="doc-biblioref">[4]</a></span> while maintaining a practical frame rate of 5fps on a GPU.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-ren2015fasterrcnn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">S. Ren, K. He, R. Girshick, and J. Sun, <span>“Faster r-CNN: Towards real-time object detection with region proposal networks,”</span> in <em>Advances in neural information processing systems</em>, 2015, pp. 91–99.</div>
</div></div></section>
</section>
<section id="mask-r-cnn" class="level2 page-columns page-full" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="mask-r-cnn"><span class="header-section-number">2.4</span> Mask R-CNN</h2>
<p>Mask R-CNN extends Faster R-CNN by adding a branch to predict the segmentation mask at each Region of Interest (RoI), alongside existing branches for classification and bounding box regression. This is achieved by introducing a small, fully convolutional network on top of each RoI. Below we briefly describe the architecture components.</p>
<section id="backbone-cnn" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="backbone-cnn"><span class="header-section-number">2.4.1</span> Backbone CNN</h3>
<p>Similar to Faster R-CNN, Mask R-CNN begins with a backbone that extracts features from the input image. This backbone network is typically pre-trained on a large dataset like ImageNet to learn generic image features.</p>
</section>
<section id="region-proposal-network-rpn-1" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="region-proposal-network-rpn-1"><span class="header-section-number">2.4.2</span> Region Proposal Network (RPN)</h3>
<p>The RPN takes feature maps from the backbone network and generates region proposals using anchor boxes, refined based on their likelihood of containing objects. The region proposals generated by the RPN are subsequently used for both object detection, and instance segmentation in the Mask R-CNN framework.</p>
</section>
<section id="region-of-interest-align-roialign" class="level3 page-columns page-full" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="region-of-interest-align-roialign"><span class="header-section-number">2.4.3</span> Region of interest Align (RoIAlign)</h3>
<p>For each region proposal generated by the RPN, features are extracted from the feature maps using RoIAlign. RoIAlign is a technique used in CNNs for object detection tasks. RoIAlign improves upon RoIPool, a previous method, by eliminating the quantization step in the pooling operation, resulting in more accurate feature extraction from regions of interest <span class="citation" data-cites="he2017maskrcnn"><a href="references.html#ref-he2017maskrcnn" role="doc-biblioref">[6]</a></span>. It precisely aligns features extracted from arbitrary-shaped regions with the spatial layout of the feature map, enabling more accurate object localization and segmentation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2017maskrcnn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">K. He, G. Gkioxari, P. Dollár, and R. Girshick, <span>“Mask r-CNN,”</span> in <em>Proceedings of the IEEE international conference on computer vision (ICCV)</em>, 2017, pp. 2961–2969. doi: <a href="https://doi.org/10.1109/ICCV.2017.322">10.1109/ICCV.2017.322</a></div>
</div></div><p>The RoIAlign layer extracts features from each region proposal, preserving spatial information better than previous pooling methods, ensuring accurate alignment of features with the region of interest.</p>
</section>
<section id="parallel-branches" class="level3 page-columns page-full" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="parallel-branches"><span class="header-section-number">2.4.4</span> Parallel Branches</h3>
<p>Mask R-CNN introduces parallel branches for object detection and instance segmentation.</p>
<ul>
<li>Object Detection Branch: Determines the class of each object within the region proposal and refines bounding box coordinates through classification and regression. – Mask Prediction Branch: Predicts segmentation masks for each object within the region proposal, generating pixel-level masks for object instances using a small fully convolutional network applied to each RoI.</li>
<li>Loss Functions: Mask R-CNN employs a multi-function loss function that combines object detection (classification and bounding box regression) and instance segmentation (mask prediction) losses, weighted by hyperparameters.</li>
</ul>
<p><span class="math display">\[
L = L_{cls} + L_{box} + L_{mask}
\]</span></p>
<ul>
<li>Training: The entire Mask R-CNN network is trained end-to-end using backpropagation with stochastic gradient descent (SGD) or other optimization algorithms, typically starting with pre-trained weights and fine-tuning for the specific task.</li>
</ul>
<div class="quarto-figure quarto-figure-center page-columns page-full" style="text-align:center;">
<figure class="figure page-columns page-full">
<p><img src="images/architecture/Mask-R-CNN.png" class="img-fluid figure-img" alt="Mask R-CNN framework with RoIAlign"></p>
<figcaption class="figure-caption">Mask R-CNN framework with RoIAlign <span class="citation" data-cites="he2020maskrcnn"><a href="references.html#ref-he2020maskrcnn" role="doc-biblioref">[7]</a></span></figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-he2020maskrcnn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">K. He, G. Gkioxari, P. Dollár, and R. Girshick, <span>“Mask r-CNN,”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 42, no. 2, pp. 386–397, Feb. 2020, doi: <a href="https://doi.org/10.1109/TPAMI.2018.2844175">10.1109/TPAMI.2018.2844175</a></div>
</div></div></figure>
</div>
<div style="page-break-after: always;"></div>
</section>
<section id="advantages-over-faster-r-cnn" class="level3" data-number="2.4.5">
<h3 data-number="2.4.5" class="anchored" data-anchor-id="advantages-over-faster-r-cnn"><span class="header-section-number">2.4.5</span> Advantages over Faster R-CNN</h3>
<ul>
<li>Offers pixel-level segmentation alongside object detection and bounding box regression.</li>
<li>Facilitates instance segmentation, segmenting each object instance in an image separately.</li>
<li>Enhances understanding of object shapes and boundaries with finer detail.</li>
<li>Despite increased complexity, maintains comparable speed and efficiency, particularly in scenarios necessitating instance-level segmentation.</li>
</ul>
<p>Overall, Mask R-CNN represents a substantial advancement over previous methods by integrating object detection and instance segmentation into a unified architecture, rendering it a versatile solution for diverse computer vision applications.</p>
</section>
</section>
<section id="yolo-you-only-look-once" class="level2 page-columns page-full" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="yolo-you-only-look-once"><span class="header-section-number">2.5</span> YOLO (You Only Look Once)</h2>
<p>YOLO (You Only Look Once) is a popular object detection algorithm that revolutionized the field of computer vision due to its speed and accuracy <span class="citation" data-cites="redmon2018yolov3"><a href="references.html#ref-redmon2018yolov3" role="doc-biblioref">[8]</a></span>. YOLO processes images in a single pass through a neural network, enabling real-time object detection. YOLOv8 is one of the iterations of the YOLO algorithm, incorporating various improvements over its predecessors to enhance performance and efficiency.</p>
<div class="no-row-height column-margin column-container"><div id="ref-redmon2018yolov3" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">J. Redmon and A. Farhadi, <span>“Yolov3: An incremental improvement,”</span> <em>arXiv preprint arXiv:1804.02767</em>, 2018.</div>
</div></div><section id="architecture-1" class="level3 page-columns page-full" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="architecture-1"><span class="header-section-number">2.5.1</span> Architecture</h3>
<p>The architecture of YOLO consists of a backbone network, detection head, and output layers. The backbone network, typically based on Darknet or CSPDarknet, extracts features from the input image <span class="citation" data-cites="bochkovskiy2020yolov4"><a href="references.html#ref-bochkovskiy2020yolov4" role="doc-biblioref">[9]</a></span>. These features are then processed by the detection head, which predicts bounding boxes, confidence scores, and class probabilities for detected objects. Earlier YOLO models utilizes anchor boxes <span class="citation" data-cites="bochkovskiy2020yolov4"><a href="references.html#ref-bochkovskiy2020yolov4" role="doc-biblioref">[9]</a></span> to improve localization accuracy and efficiency. Finally, the output layers produce the final detection and confidence scores.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bochkovskiy2020yolov4" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">A. Bochkovskiy <em>et al.</em>, <span>“YOLOv4: Optimal speed and accuracy of object detection,”</span> <em>arXiv preprint arXiv:2004.10934</em>, 2020.</div>
</div></div><div class="quarto-figure quarto-figure-center page-columns page-full" style="text-align:center;">
<figure class="figure page-columns page-full">
<p><img src="images/architecture/yolov8-architecture.jpg" class="img-fluid figure-img" alt="Yolov8 architecture"></p>
<figcaption class="figure-caption">YOLOv8 architecture <span class="citation" data-cites="zhang2023dcf-yolov8"><a href="references.html#ref-zhang2023dcf-yolov8" role="doc-biblioref">[10]</a></span></figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2023dcf-yolov8" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">L. Zhang, G. Ding, C. Li, and D. Li, <span>“DCF-Yolov8: An improved algorithm for aggregating low-level features to detect agricultural pests and diseases,”</span> <em>Agronomy</em>, vol. 13, p. 2012, Jul. 2023, doi: <a href="https://doi.org/10.3390/agronomy13082012">10.3390/agronomy13082012</a></div>
</div></div></figure>
</div>
</section>
<section id="anchor-free-approach" class="level3 page-columns page-full" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="anchor-free-approach"><span class="header-section-number">2.5.2</span> Anchor-Free Approach</h3>
<p>YOLOv8 employs an anchor-free approach <span class="citation" data-cites="yolov8_ultralytics"><a href="references.html#ref-yolov8_ultralytics" role="doc-biblioref">[11]</a></span>, eliminating the need for predefined anchor boxes. This simplifies the training process and improves model flexibility.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="efficient-backbone" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="efficient-backbone"><span class="header-section-number">2.5.3</span> Efficient Backbone</h3>
<p>YOLO utilizes lightweight backbone networks such as CSPDarknet53, which strikes a balance between performance and computational efficiency</p>
</section>
<section id="improved-object-detection" class="level3 page-columns page-full" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="improved-object-detection"><span class="header-section-number">2.5.4</span> Improved Object Detection</h3>
<p>YOLO incorporates advanced techniques such as focal loss, which helps to address class imbalance and improve object detection accuracy <span class="citation" data-cites="tian2019fcos"><a href="references.html#ref-tian2019fcos" role="doc-biblioref">[12]</a></span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tian2019fcos" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">Z. Tian, C. Shen, H. Chen, and T. He, <span>“FCOS: Fully convolutional one-stage object detection,”</span> <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp. 9627–9636, 2019.</div>
</div></div></section>
<section id="real-time-performance" class="level3 page-columns page-full" data-number="2.5.5">
<h3 data-number="2.5.5" class="anchored" data-anchor-id="real-time-performance"><span class="header-section-number">2.5.5</span> Real-Time Performance</h3>
<p>YOLOv8 is optimized for real-time object detection applications <span class="citation" data-cites="yolov8_ultralytics"><a href="references.html#ref-yolov8_ultralytics" role="doc-biblioref">[11]</a></span>, offering fast inference speeds without compromising accuracy.</p>
<div class="no-row-height column-margin column-container"><div id="ref-yolov8_ultralytics" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">G. Jocher, A. Chaurasia, and J. Qiu, <span>“Ultralytics YOLOv8.”</span> 2023. Available: <a href="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</a></div>
</div></div></section>
<section id="advantages-over-mask-r-cnn" class="level3 page-columns page-full" data-number="2.5.6">
<h3 data-number="2.5.6" class="anchored" data-anchor-id="advantages-over-mask-r-cnn"><span class="header-section-number">2.5.6</span> Advantages over Mask R-CNN</h3>
<p>Both Mask R-CNN and YOLOv8 are instance segmentation models; however, Mask R-CNN is better suited for semantic segmentation while YOLOv8 is not initially designed for that purpose. Both models are pre-trained on the MS-COCO dataset. Mask R-CNN is an improved version of Faster R-CNN and uses ResNet-101 as its backbone and has the addition of the prediction of object masks. YOLOv8 uses CSPDarknet53 as its backbone, which has 53 convolutional layers.</p>
<p>The main difference between YOLOv8 and Mask R-CNN is the region proposal network. YOLOv8 is an anchor free model so it does not need a region proposal network. Another difference between the two models is that Mask R-CNN is a two stage model and YOLOv8 is a one stage model. The first stage of Mask R-CNN is the application of the region proposal network and the second stage executes “bounding box regression, classification, and mask prediction” <span class="citation" data-cites="ameli2024"><a href="references.html#ref-ameli2024" role="doc-biblioref">[13]</a></span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ameli2024" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">Z. Ameli, S. J. Nesheli, and E. N. Landis, <span>“Deep learning-based steel bridge corrosion segmentation and condition rating using mask RCNN and YOLOv8,”</span> <em>Infrastructures</em>, vol. 9, no. 1, 2024, doi: <a href="https://doi.org/10.3390/infrastructures9010003">10.3390/infrastructures9010003</a>. Available: <a href="https://www.mdpi.com/2412-3811/9/1/3">https://www.mdpi.com/2412-3811/9/1/3</a></div>
</div></div><p>On the other hand, YOLOv8 has one stage that consists of “directly predicting bounding boxes and class labels for objects in an image” <span class="citation" data-cites="ameli2024"><a href="references.html#ref-ameli2024" role="doc-biblioref">[13]</a></span>. YOLOv8 and Mask R-CNN also differ in their loss functions. YOLOv8 uses complete intersection over union loss, distribution focal loss, and binary cross entropy. Mask R-CNN uses classification loss, regression loss, and mask loss. In addition YOLOv8 is best used for real time object detection with low latency <span class="citation" data-cites="ameli2024"><a href="references.html#ref-ameli2024" role="doc-biblioref">[13]</a></span>.&nbsp;</p>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./intro.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./methodology.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Methodologies</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>