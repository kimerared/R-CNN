<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computer Vision Object Detection - 3&nbsp; Literature Review</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./summary.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Literature-Review.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Literature Review</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computer Vision Object Detection</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Literature-Review.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Literature Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#r-cnn-region-based-convolutional-network" id="toc-r-cnn-region-based-convolutional-network" class="nav-link active" data-scroll-target="#r-cnn-region-based-convolutional-network"><span class="header-section-number">3.1</span> R-CNN (Region-Based Convolutional Network)</a>
  <ul class="collapse">
  <li><a href="#region-proposal-generation" id="toc-region-proposal-generation" class="nav-link" data-scroll-target="#region-proposal-generation"><span class="header-section-number">3.1.1</span> Region Proposal Generation</a></li>
  <li><a href="#feature-extraction" id="toc-feature-extraction" class="nav-link" data-scroll-target="#feature-extraction"><span class="header-section-number">3.1.2</span> Feature Extraction</a></li>
  <li><a href="#fine-tuning-and-classification" id="toc-fine-tuning-and-classification" class="nav-link" data-scroll-target="#fine-tuning-and-classification"><span class="header-section-number">3.1.3</span> Fine-tuning and Classification</a></li>
  <li><a href="#bounding-box-regression" id="toc-bounding-box-regression" class="nav-link" data-scroll-target="#bounding-box-regression"><span class="header-section-number">3.1.4</span> Bounding Box Regression</a></li>
  <li><a href="#non-maximum-suppression" id="toc-non-maximum-suppression" class="nav-link" data-scroll-target="#non-maximum-suppression"><span class="header-section-number">3.1.5</span> Non-Maximum Suppression</a></li>
  <li><a href="#drawbacks" id="toc-drawbacks" class="nav-link" data-scroll-target="#drawbacks"><span class="header-section-number">3.1.6</span> Drawbacks</a></li>
  </ul></li>
  <li><a href="#fast-r-cnn" id="toc-fast-r-cnn" class="nav-link" data-scroll-target="#fast-r-cnn"><span class="header-section-number">3.2</span> Fast R-CNN</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture"><span class="header-section-number">3.2.1</span> Architecture</a></li>
  <li><a href="#feature-extraction-with-roi-pooling" id="toc-feature-extraction-with-roi-pooling" class="nav-link" data-scroll-target="#feature-extraction-with-roi-pooling"><span class="header-section-number">3.2.2</span> Feature Extraction with RoI Pooling</a></li>
  <li><a href="#advantages-of-fast-r-cnn" id="toc-advantages-of-fast-r-cnn" class="nav-link" data-scroll-target="#advantages-of-fast-r-cnn"><span class="header-section-number">3.2.3</span> Advantages of Fast R-CNN</a></li>
  </ul></li>
  <li><a href="#faster-r-cnn" id="toc-faster-r-cnn" class="nav-link" data-scroll-target="#faster-r-cnn"><span class="header-section-number">3.3</span> Faster R-CNN</a>
  <ul class="collapse">
  <li><a href="#region-proposal-network-rpn" id="toc-region-proposal-network-rpn" class="nav-link" data-scroll-target="#region-proposal-network-rpn"><span class="header-section-number">3.3.1</span> Region Proposal network (RPN)</a></li>
  <li><a href="#anchor-boxes" id="toc-anchor-boxes" class="nav-link" data-scroll-target="#anchor-boxes"><span class="header-section-number">3.3.2</span> Anchor Boxes</a></li>
  <li><a href="#region-of-interest-roi-pooling-or-roialign" id="toc-region-of-interest-roi-pooling-or-roialign" class="nav-link" data-scroll-target="#region-of-interest-roi-pooling-or-roialign"><span class="header-section-number">3.3.3</span> Region of Interest (RoI) Pooling or RoIAlign</a></li>
  <li><a href="#classifier-and-bounding-box-regressor" id="toc-classifier-and-bounding-box-regressor" class="nav-link" data-scroll-target="#classifier-and-bounding-box-regressor"><span class="header-section-number">3.3.4</span> Classifier and Bounding Box Regressor</a></li>
  <li><a href="#non-maximum-suppression-nms" id="toc-non-maximum-suppression-nms" class="nav-link" data-scroll-target="#non-maximum-suppression-nms"><span class="header-section-number">3.3.5</span> Non-Maximum Suppression (NMS)</a></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output"><span class="header-section-number">3.3.6</span> Output</a></li>
  <li><a href="#advantages-of-faster-r-cnn" id="toc-advantages-of-faster-r-cnn" class="nav-link" data-scroll-target="#advantages-of-faster-r-cnn"><span class="header-section-number">3.3.7</span> Advantages of Faster R-CNN</a></li>
  </ul></li>
  <li><a href="#mask-r-cnn" id="toc-mask-r-cnn" class="nav-link" data-scroll-target="#mask-r-cnn"><span class="header-section-number">3.4</span> Mask R-CNN</a>
  <ul class="collapse">
  <li><a href="#architecture-1" id="toc-architecture-1" class="nav-link" data-scroll-target="#architecture-1"><span class="header-section-number">3.4.1</span> Architecture</a></li>
  <li><a href="#backbone-cnn" id="toc-backbone-cnn" class="nav-link" data-scroll-target="#backbone-cnn"><span class="header-section-number">3.4.2</span> Backbone CNN</a></li>
  <li><a href="#regional-proposal-network-rpn" id="toc-regional-proposal-network-rpn" class="nav-link" data-scroll-target="#regional-proposal-network-rpn"><span class="header-section-number">3.4.3</span> Regional Proposal Network (RPN)</a></li>
  <li><a href="#roi-align" id="toc-roi-align" class="nav-link" data-scroll-target="#roi-align"><span class="header-section-number">3.4.4</span> RoI Align</a></li>
  <li><a href="#parallel-branches" id="toc-parallel-branches" class="nav-link" data-scroll-target="#parallel-branches"><span class="header-section-number">3.4.5</span> Parallel Branches</a></li>
  <li><a href="#advantages-over-faster-r-cnn" id="toc-advantages-over-faster-r-cnn" class="nav-link" data-scroll-target="#advantages-over-faster-r-cnn"><span class="header-section-number">3.4.6</span> Advantages over Faster R-CNN</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Literature Review</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The evolution from R-CNN to faster R-CNN represents a significant advancement in object detection algorithms, especially in speed and efficiency. A brief history of the development progression of R-CNN, Fast R-CNN, and Faster R-CNN adds valuable context to our study on object detection architectures. It will help to understand the evolution of these models, the motivations behind their development, and the improvements made over time.</p>
<section id="r-cnn-region-based-convolutional-network" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="r-cnn-region-based-convolutional-network"><span class="header-section-number">3.1</span> R-CNN (Region-Based Convolutional Network)</h2>
<p>R-CNN was a breakthrough in object detection. It employed a multi-stage approach that involved a selective search for generating region proposals followed by a convolutional neural network for feature extraction and a support vector machine (SVM) for object classification within each region.</p>
<section id="region-proposal-generation" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="region-proposal-generation"><span class="header-section-number">3.1.1</span> Region Proposal Generation</h3>
<p>R-CNN began with generating region proposals using the selective search algorithm <span class="citation" data-cites="girshick2014rich"><a href="references.html#ref-girshick2014rich" role="doc-biblioref">[1]</a></span>. Selective search is a method for identifying potential object regions in an image based on low-level features such as color, texture, and intensity. It produces a set of bounding boxes that mostly have objects.</p>
</section>
<section id="feature-extraction" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="feature-extraction"><span class="header-section-number">3.1.2</span> Feature Extraction</h3>
<p>Following the generation of region proposals, R-CNN employed a pre-trained convolutional neural network to independently extract features from each region. Typically, the chosen CNN model was AlexNet, pre-trained on the ImageNet dataset specifically for image classification tasks.</p>
</section>
<section id="fine-tuning-and-classification" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="fine-tuning-and-classification"><span class="header-section-number">3.1.3</span> Fine-tuning and Classification</h3>
<p>Following feature extraction, the extracted features were input into a distinct classifier to ascertain the presence of objects within the regions. R-CNN utilized a support vector machine (SVM) for this classification task. Each SVM was trained to discern whether the feature corresponded to a particular object or background.</p>
</section>
<section id="bounding-box-regression" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="bounding-box-regression"><span class="header-section-number">3.1.4</span> Bounding Box Regression</h3>
<p>Following classification, R-CNN conducted bounding box regression to enhance the accuracy of the detected object locations. This process adjusts the bounding boxes produced by the region proposal algorithm to align more precisely with the actual object locations within the regions.</p>
</section>
<section id="non-maximum-suppression" class="level3" data-number="3.1.5">
<h3 data-number="3.1.5" class="anchored" data-anchor-id="non-maximum-suppression"><span class="header-section-number">3.1.5</span> Non-Maximum Suppression</h3>
<p>Lastly, R-CNN implemented non-maximum suppression to eliminate redundant detections, ensuring that each object is detected only once.</p>
<div class="quarto-figure quarto-figure-center" style="text-align:center;">
<figure class="figure">
<p><img src="images/LR/LR-fig01-R-CNN-flow.png" class="img-fluid figure-img" alt="R-CNN Flowchart"></p>
<figcaption class="figure-caption">Figure 1: Flowchart of R-CNN</figcaption>
</figure>
</div>
</section>
<section id="drawbacks" class="level3" data-number="3.1.6">
<h3 data-number="3.1.6" class="anchored" data-anchor-id="drawbacks"><span class="header-section-number">3.1.6</span> Drawbacks</h3>
<p>An initial drawback of the R-CNN architecture was its computational inefficiency during inference, primarily stemming from its sequential processing of region proposals. Processing each region proposal independently led to redundant computations and prolonged inference times.</p>
</section>
</section>
<section id="fast-r-cnn" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="fast-r-cnn"><span class="header-section-number">3.2</span> Fast R-CNN</h2>
<p>Fast R-CNN represents a faster and more efficient iteration of the original R-CNN for object detection. It tackles the computational inefficiencies of R-CNN by introducing a unified architecture that consolidates region proposal generation, feature extraction, and object classification into a single network. This approach dramatically reduces redundant computations and accelerates the inference process.</p>
<section id="architecture" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="architecture"><span class="header-section-number">3.2.1</span> Architecture</h3>
<p>Fast R-CNN employs a regional pattern network (RPN) to directly generate region proposals from the convolutional feature maps of the input image <span class="citation" data-cites="girshick2015fastrcnn"><a href="references.html#ref-girshick2015fastrcnn" role="doc-biblioref">[2]</a></span>. This eliminates the necessity for external region proposals, such as selective search, as used in the original R-CNN. The RPN functions by sliding a small grid (essentially a compact CNN) across the evolving feature map to predict the spatial dimensions (bounding boxes) and associated probability scores for objects within each sliding window. Subsequently, these local parameters serve as inputs for the subsequent stage of the process.</p>
</section>
<section id="feature-extraction-with-roi-pooling" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="feature-extraction-with-roi-pooling"><span class="header-section-number">3.2.2</span> Feature Extraction with RoI Pooling</h3>
<p>The RoI pooling layer employs max pooling to transform features within any valid region of interest into a compact feature map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters independent of any specific RoI. A more efficient training strategy leverages feature sharing throughout the process. During Fast R-CNN training, stochastic gradient descent (SGD) minibatches are hierarchically sampled: N images are initially sampled, followed by R/N RoIs from each image. Notably, both during forward and backward passes, ROIs from the same image share computation and memory. Fixed-size feature maps yielded from RoI pooling are input to fully connected layers for object classification and bounding-box regression. Feature classification aims to detect features and assign class probabilities for each proposed location, while bounding box regression seeks to enhance the localization accuracy of detected features. Fast R-CNN introduces multitasking loss functions that amalgamate classification and bounding box regression losses. This enables joint training of both tasks, ensuring the network learns to predict object location and precise bounding boxes concurrently.</p>
<div class="quarto-figure quarto-figure-center" style="text-align: center;">
<figure class="figure">
<p><img src="images/LR/LR-fig02-Fast-R-CNN.png" class="img-fluid figure-img" alt="Fast R-CNN Flowchart"></p>
<figcaption class="figure-caption">Figure 2: Flowchart of Fast R-CNN</figcaption>
</figure>
</div>
</section>
<section id="advantages-of-fast-r-cnn" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="advantages-of-fast-r-cnn"><span class="header-section-number">3.2.3</span> Advantages of Fast R-CNN</h3>
<p>In contrast to R-CNN, which comprises multiple independent steps (such as region proposal generation, feature extraction, classification, and bounding box regression), Fast R-CNN integrates these processes within a unified network architecture. This enables end-to-end training of the entire pipeline, leading to improved optimization and potentially higher accuracy. Fast R-CNN achieves variable sharing across all spatial dimensions of an image. Unlike R-CNN, which extracts features independently for each image, Fast R-CNN performs feature extraction on the entire image only once. This shared computation across convolutional features significantly minimizes redundant calculations, expediting the inference process.</p>
</section>
</section>
<section id="faster-r-cnn" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="faster-r-cnn"><span class="header-section-number">3.3</span> Faster R-CNN</h2>
<p>Compared to preceding methodologies, Faster R-CNN achieves swifter and more efficient object recognition by seamlessly integrating regional proposal steps directly into the network architecture. Subsequent advancements have expanded upon this framework, establishing it as a fundamental component in object recognition. The architecture commences with a feature extraction backbone, typically a convolutional neural network (CNN) pre-trained on extensive datasets such as ImageNet. This backbone extracts pertinent features from the input image.</p>
<section id="region-proposal-network-rpn" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="region-proposal-network-rpn"><span class="header-section-number">3.3.1</span> Region Proposal network (RPN)</h3>
<p>The feature maps derived from the backbone network are input to a Region Proposal Network (RPN) <span class="citation" data-cites="ren2015fasterrcnn"><a href="references.html#ref-ren2015fasterrcnn" role="doc-biblioref">[3]</a></span>. The RPN, a compact fully convolutional network, employs a sliding window approach (typically 3x3) over the feature maps to generate region proposals. The RPN yields a collection of bounding box proposals accompanied by objectness scores, indicating the probability of containing an object. These proposals are generated by leveraging predefined anchor boxes of varying scales and aspect ratios.</p>
</section>
<section id="anchor-boxes" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="anchor-boxes"><span class="header-section-number">3.3.2</span> Anchor Boxes</h3>
<p>The RPN generates region proposals by predicting offsets and scales for a predefined set of anchor boxes at each spatial position in the feature maps. These anchor boxes, varying in size and aspect ratio, serve as reference boxes for the proposal generation process.</p>
</section>
<section id="region-of-interest-roi-pooling-or-roialign" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="region-of-interest-roi-pooling-or-roialign"><span class="header-section-number">3.3.3</span> Region of Interest (RoI) Pooling or RoIAlign</h3>
<p>For each region proposal generated by the RPN, features are extracted from the feature maps using RoI pooling or RoIAlign. RoI pooling or RoIAlign ensures that features from the region proposals are aligned and resized to a fixed size, facilitating further processing.</p>
</section>
<section id="classifier-and-bounding-box-regressor" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="classifier-and-bounding-box-regressor"><span class="header-section-number">3.3.4</span> Classifier and Bounding Box Regressor</h3>
<p>The RoI-pooled or RoIAlign features are separately input into branches for classification and bounding box regression.</p>
<ul>
<li><strong>Classification:</strong> The features traverse a classifier (e.g., fully connected layers followed by softmax) to predict the probability of each region proposal belonging to different object classes. Bounding Box Regression: Another set of fully connected layers predicts refined bounding box coordinates for each region proposal.</li>
<li><strong>Loss Function:</strong> The network undergoes end-to-end training using a multi-task loss function, which combines losses from the RPN (objectness score prediction and bounding box regression) with those from the classification and bounding box regression branches.</li>
</ul>
</section>
<section id="non-maximum-suppression-nms" class="level3" data-number="3.3.5">
<h3 data-number="3.3.5" class="anchored" data-anchor-id="non-maximum-suppression-nms"><span class="header-section-number">3.3.5</span> Non-Maximum Suppression (NMS)</h3>
<p>Following prediction, non-maximum suppression is employed to discard redundant and overlapping detections based on their confidence scores and bounding box coordinates.</p>
</section>
<section id="output" class="level3" data-number="3.3.6">
<h3 data-number="3.3.6" class="anchored" data-anchor-id="output"><span class="header-section-number">3.3.6</span> Output</h3>
<p>The final output comprises a collection of object detections alongside their bounding boxes and corresponding class labels.</p>
<div class="quarto-figure quarto-figure-center" style="text-align:center;">
<figure class="figure">
<p><img src="images/LR/LR-fig03-Faster-R-CNN.png" class="img-fluid figure-img" alt="Faster R-CNN Flowchart"></p>
<figcaption class="figure-caption">Figure 3: Flowchart of Faster R-CNN <span class="citation" data-cites="ren2015fasterrcnn"><a href="references.html#ref-ren2015fasterrcnn" role="doc-biblioref">[3]</a></span></figcaption>
</figure>
</div>
</section>
<section id="advantages-of-faster-r-cnn" class="level3" data-number="3.3.7">
<h3 data-number="3.3.7" class="anchored" data-anchor-id="advantages-of-faster-r-cnn"><span class="header-section-number">3.3.7</span> Advantages of Faster R-CNN</h3>
<ul>
<li><strong>Efficient Region Proposal Generation:</strong> The Region Proposal Network (RPN) significantly reduces the computational load of region proposal methods, enabling nearly cost-free region proposals by sharing convolutional features.</li>
<li><strong>End-to-End Training:</strong> RPN and Fast R-CNN can be jointly trained to share convolutional features, leading to a unified network architecture.</li>
<li><strong>High-Quality Proposals:</strong> RPN produces high-quality region proposals, enhancing detection accuracy compared to traditional methods like Selective Search and EdgeBoxes.</li>
<li><strong>Practical Implementation:</strong> The proposed method achieves state-of-the-art object detection accuracy on benchmarks like PASCAL VOC while maintaining a practical frame rate of 5fps on a GPU.</li>
</ul>
</section>
</section>
<section id="mask-r-cnn" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="mask-r-cnn"><span class="header-section-number">3.4</span> Mask R-CNN</h2>
<p>Mask R-CNN extends Faster R-CNN by adding a branch to predict the segmentation mask at each Region of Interest (RoI), alongside existing branches for classification and bounding box regression. This is achieved by introducing a small, fully convolutional network on top of each RoI.</p>
<section id="architecture-1" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="architecture-1"><span class="header-section-number">3.4.1</span> Architecture</h3>
</section>
<section id="backbone-cnn" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="backbone-cnn"><span class="header-section-number">3.4.2</span> Backbone CNN</h3>
<p>Similar to Faster R-CNN, Mask R-CNN begins with a backbone Convolutional Neural Network (CNN) that extracts features from the input image. This backbone network is typically pre-trained on a large dataset like ImageNet to learn generic image features.</p>
</section>
<section id="regional-proposal-network-rpn" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="regional-proposal-network-rpn"><span class="header-section-number">3.4.3</span> Regional Proposal Network (RPN)</h3>
<p>The RPN takes feature maps from the backbone network and generates region proposals using anchor boxes, refined based on their likelihood of containing objects.</p>
</section>
<section id="roi-align" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="roi-align"><span class="header-section-number">3.4.4</span> RoI Align</h3>
<p>The RoI align layer extracts features from each region proposal, preserving spatial information better than previous pooling methods, ensuring accurate alignment of features with the region of interest.</p>
</section>
<section id="parallel-branches" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="parallel-branches"><span class="header-section-number">3.4.5</span> Parallel Branches</h3>
<p>Mask R-CNN introduces parallel branches for object detection and instance segmentation.</p>
<ul>
<li>Object Detection Branch: Determines the class of each object within the region proposal and refines bounding box coordinates through classification and regression. – Mask Prediction Branch: Predicts segmentation masks for each object within the region proposal, generating pixel-level masks for object instances using a small fully convolutional network applied to each RoI.</li>
<li>Loss Functions: Mask R-CNN employs a multi-function loss function that combines object detection (classification and bounding box regression) and instance segmentation (mask prediction) losses, weighted by hyperparameters.</li>
</ul>
<p><span class="math display">\[
L = L_{cls} + L_{box} + L_{mask}
\]</span></p>
<ul>
<li>Training: The entire Mask R-CNN network is trained end-to-end using backpropagation with stochastic gradient descent (SGD) or other optimization algorithms, typically starting with pre-trained weights and fine-tuning for the specific task.</li>
</ul>
<div class="quarto-figure quarto-figure-center" style="text-align:center;">
<figure class="figure">
<p><img src="images/LR/LR-fig04-Mask-R-CNN.png" class="img-fluid figure-img" alt="Mask R-CNN framework for Instance Segmentation"></p>
<figcaption class="figure-caption">Figure 4: Mask R-CNN framework for Instance Segmentation <span class="citation" data-cites="he2020maskrcnn"><a href="references.html#ref-he2020maskrcnn" role="doc-biblioref">[4]</a></span></figcaption>
</figure>
</div>
</section>
<section id="advantages-over-faster-r-cnn" class="level3" data-number="3.4.6">
<h3 data-number="3.4.6" class="anchored" data-anchor-id="advantages-over-faster-r-cnn"><span class="header-section-number">3.4.6</span> Advantages over Faster R-CNN</h3>
<ul>
<li>Offers pixel-level segmentation alongside object detection and bounding box regression.</li>
<li>Facilitates instance segmentation, segmenting each object instance in an image separately.</li>
<li>Enhances understanding of object shapes and boundaries with finer detail.</li>
<li>Despite increased complexity, maintains comparable speed and efficiency, particularly in scenarios necessitating instance-level segmentation.</li>
</ul>
<p>Overall, Mask R-CNN represents a substantial advancement over previous methods by integrating object detection and instance segmentation into a unified architecture, rendering it a versatile solution for diverse computer vision applications.</p>


<div id="refs" class="references csl-bib-body" role="list" style="display: none">
<div id="ref-girshick2014rich" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">R. Girshick, J. Donahue, T. Darrell, and J. Malik, <span>“Rich feature hierarchies for accurate object detection and semantic segmentation,”</span> in <em>2014 IEEE conference on computer vision and pattern recognition</em>, 2014, pp. 580–587. doi: <a href="https://doi.org/10.1109/CVPR.2014.81">10.1109/CVPR.2014.81</a></div>
</div>
<div id="ref-girshick2015fastrcnn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">R. Girshick, <span>“Fast r-CNN,”</span> in <em>2015 IEEE international conference on computer vision (ICCV)</em>, 2015, pp. 1440–1448. doi: <a href="https://doi.org/10.1109/ICCV.2015.169">10.1109/ICCV.2015.169</a></div>
</div>
<div id="ref-ren2015fasterrcnn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">S. Ren, K. He, R. Girshick, and J. Sun, <span>“Faster r-CNN: Towards real-time object detection with region proposal networks,”</span> in <em>Advances in neural information processing systems</em>, 2015.</div>
</div>
<div id="ref-he2020maskrcnn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">K. He, G. Gkioxari, P. Dollár, and R. Girshick, <span>“Mask r-CNN,”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 42, no. 2, pp. 386–397, Feb. 2020, doi: <a href="https://doi.org/10.1109/TPAMI.2018.2844175">10.1109/TPAMI.2018.2844175</a></div>
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./summary.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Summary</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>