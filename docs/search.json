[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer Vision Object Detection",
    "section": "",
    "text": "Objectives\nThe initial objective of this research paper is to evaluate the performance of the Faster R-CNN methodology on a dataset provided by Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun in their paper titled “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” introduced at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) in 2015 [1]. We aim to replicate the results presented in this article, and gain a comprehensive understanding of Faster R-CNN, which will serve as the foundation for our future testing endeavors. Furthermore, we will assess the performance of Mask R-CNN, and YOLOv8 to offer a comparison of various object detection methodologies. A custom class label was applied to detect military tanks using a YOLOv8 model.\n\n\nGlossary\n\n\n\nTerm\nDefinition\n\n\n\n\nCNN\nConvolutional Neural Network\n\n\nR-CNN\nRegion-based Convolutional Neural Network\n\n\nYOLOv8\nYou Only Look Once, version 8\n\n\nRPN\nRegion Proposal Network\n\n\nIoU\nIntersection over Union\n\n\nRoI\nRegion of Interest\n\n\nRoIAlign\nRegion of Interest Align\n\n\nGPU\nGraphics Processing Unit\n\n\nCPU\nCentral Processing Unit\n\n\nReLU\nRectified Linear Unit\n\n\nSVM\nSupport Vector Machine\n\n\nmAP\nmean Average Precision\n\n\nCOCO\nMicrosoft Common Objects in Context\n\n\n\n\n\n\n\n\n\n\n[1] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-CNN: Towards real-time object detection with region proposal networks,” in Advances in neural information processing systems, 2015, pp. 91–99."
  },
  {
    "objectID": "abstract.html",
    "href": "abstract.html",
    "title": "1  Abstract",
    "section": "",
    "text": "The faster R-CNN approach significantly enhances object recognition. It meets the requirement for high precision near real-time detection. The Regional Pattern Network (RPN), Region of Interest (RoI) Network, Backbone Convolutional Neural Network (CNN), and Training/Pattern Framework are the primary components of the rapid R-CNN technique.\nProposal Network (RPN) is a significant advancement in Faster R-CNN technology. The convolutional feature map is used to efficiently produce regional dimensions. This leads to a considerably more versatile, end-to-end trainable system thas does not require external recommendation techniques like selective search.\nUpon obtaining the local parameters, identically sized objects are extracted from the feature map for every local parameter using the ROI pooling layer. Then, bounding box return and classification are performed using these features. By using this method, the model may concentrate on the areas of the picture that are most important, which increases accuracy and speed.\nTo extract information from the visual input, the spinal cord’s Convolutional Neural Network (CNN) is employed. The most widely used options for spinal networks are ResNet and VGG-16. The model gains from transfer learning, and is able to extract well-formed features from the input photos by utilizing a pre-trained CNN as its foundation.\nUsing a multitasking loss function that combines classification loss with bounding box regression loss, the model is trained from beginning to end. As a result, the model may learn to predict bounding boxes for objects, and classify them concurrently. The computationally trained model is appropriate for low latency applications since it may be utilized for real-time object detection.\nThe extensively utilized Faster R-CNN technique has served as the foundation for numerous extremely sophisticated object recognition systems. It has positively contributed to different fields including, but limited to autonomous driving, robotics, medical diagnostic, manufacturing, and video surveillance."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "Object detection is a fundamental task in computer vision, playing a crucial role in various applications such as autonomous vehicles, surveillance systems, and medical imaging. Recent advancements in deep learning have led to the creation of sophisticated architectures such as Faster R-CNN, Mask R-CNN, and YOLO (You Only Look Once), all of which have substantially improved object detection performance. These architectures have revolutionized the field by achieving superior accuracy in detecting objects within images. However, despite significant advancements, a critical trade-off between accuracy and processing speed persists. It is essential to carefully consider this balance when selecting the most suitable model for practical applications.\nThe efficiency of object detection models is essential, particularly in real-time applications where rapid decision-making is vital. Currently, object detection primarily relies on region proposal methods, and region CNNs. For instance, Selective Search, a widely used region proposal method, exhibits a speed of 3.9 seconds per class per image [1] when implemented on the CPU, which is considered slow for many real-world applications. On the other hand, Edge Boxes, another region proposal method, offers improved speed, clocking in at 0.25 seconds per image [2]. However, a notable constraint remains – even with advancements, the region proposal step still demands substantial computational resources when executed on the CPU rather than the GPU.\nMoreover, comparing the performance of Faster R-CNN with region proposal methods poses challenges. Since R-CNNs leverage GPU acceleration while region proposal methods primarily rely on CPU computation. A fair comparison between the two becomes challenging. To address this issue, Region Proposal Networks (RPNs) were introduced in 2015 as part of the Faster R-CNN framework [3]. A novel approach that integrates the region proposal step directly into the object detection algorithm itself. This architectural innovation aims to streamline the object detection process and improve overall efficiency by leveraging GPU resources effectively.\nNonetheless, the extensively utilized Faster R-CNN technique has served as the cornerstone for a highly advanced object recognition systems. Its robustness, and versatility have made significant contributions across diverse fields such as autonomous driving, where it aids in the detection of pedestrians, vehicles, and obstacles, ensuring safer navigation. In robotics, Faster R-CNN facilitates object identification and localization, enabling robots to perform intricate tasks with precision and efficiency. Moreover, in the realm of medical diagnostics, this technique plays a pivotal role in analyzing medical imagery, assisting healthcare professionals in accurate disease detection and treatment planning. Additionally, in video surveillance applications, Faster R-CNN enhances security measures by identifying and tracking suspicious activities, or individuals in real-time; thus, increasing overall safety, and surveillance effectiveness. Overall, the widespread adoption of Faster R-CNN underscores its profound impact, and indispensable role in advancing various technological domains.\n\n\n\n\n[1] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders, “Selective search for object recognition,” International Journal of Computer Vision, vol. 104, no. 2, pp. 154–171, 2013.\n\n\n[2] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals from edges,” in European conference on computer vision, 2014, pp. 391–405.\n\n\n[3] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-CNN: Towards real-time object detection with region proposal networks,” in Advances in neural information processing systems, 2015, pp. 91–99."
  },
  {
    "objectID": "Literature-Review.html#r-cnn-region-based-convolutional-network",
    "href": "Literature-Review.html#r-cnn-region-based-convolutional-network",
    "title": "3  Literature Review",
    "section": "3.1 R-CNN (Region-Based Convolutional Network)",
    "text": "3.1 R-CNN (Region-Based Convolutional Network)\nR-CNN was a breakthrough in object detection. It employed a multi-stage approach that involved a selective search for generating region proposals followed by a convolutional neural network for feature extraction and a support vector machine (SVM) for object classification within each region.\n\n3.1.1 Region Proposal Generation\nR-CNN began with generating region proposals using the selective search algorithm [1]. Selective search is a method for identifying potential object regions in an image based on low-level features such as color, texture, and intensity. It produces a set of bounding boxes that mostly have objects.\n\n\n3.1.2 Feature Extraction\nFollowing the generation of region proposals, R-CNN employed a pre-trained convolutional neural network to independently extract features from each region. Typically, the chosen CNN model was AlexNet, pre-trained on the ImageNet dataset specifically for image classification tasks.\n\n\n3.1.3 Fine-tuning and Classification\nFollowing feature extraction, the extracted features were input into a distinct classifier to ascertain the presence of objects within the regions. R-CNN utilized a support vector machine (SVM) [1] for this classification task. Each SVM was trained to discern whether the feature corresponded to a particular object or background.\n\n\n3.1.4 Bounding Box Regression\nFollowing classification, R-CNN conducted bounding box regression to enhance the accuracy of the detected object locations. This process adjusts the bounding boxes produced by the region proposal algorithm to align more precisely with the actual object locations within the regions.\n\n\n3.1.5 Non-Maximum Suppression\nLastly, R-CNN implemented non-maximum suppression to eliminate redundant detections, ensuring that each object is detected only once.\n\n\n\nFlowchart of R-CNN\n\n\n\n\n3.1.6 Drawbacks\nAn initial drawback of the R-CNN architecture was its computational inefficiency during inference, primarily stemming from its sequential processing of region proposals. Processing each region proposal independently led to redundant computations and prolonged inference times."
  },
  {
    "objectID": "Literature-Review.html#fast-r-cnn",
    "href": "Literature-Review.html#fast-r-cnn",
    "title": "3  Literature Review",
    "section": "3.2 Fast R-CNN",
    "text": "3.2 Fast R-CNN\nFast R-CNN, a significant advancement in object detection, introduces several innovations enhancing both training and testing efficiency while improving detection accuracy. Unlike its predecessors, Fast R-CNN leverages the VGG16 network, achieving a remarkable 9x increase in training speed compared to R-CNN [2]. At test-time, Fast R-CNN demonstrates an impressive speed improvement of 213x, making it significantly faster and more practical for real-world applications. Additionally, Fast R-CNN outperforms previous methods in terms of mean Average Precision (mAP) on benchmark datasets like PASCAL VOC 2012 [2].\n–Fast R-CNN represents a faster and more efficient iteration of the original R-CNN for object detection. It tackles the computational inefficiencies of R-CNN by introducing a unified architecture that consolidates region proposal generation, feature extraction, and object classification into a single network. This approach dramatically reduces redundant computations and accelerates the inference process.–\n\n3.2.1 Architecture\nExpanding upon its architecture, Fast R-CNN incorporates several key components to achieve its performance gains. Fast R-CNN addresses the speed and efficiency limitations of R-CNN by proposing a unified architecture that integrates region proposal generation, feature extraction, and object classification into a single network.\nFirstly, it utilizes a Region Proposal Network (RPN) to generate region proposals directly from the convolutional feature maps, eliminating the need for external proposal methods like selective search. The RPN functions by sliding a small grid (essentially a compact CNN) across the evolving feature map to predict the spatial dimensions (bounding boxes) and associated probability scores for objects within each sliding window. This streamlines the detection process and enhances efficiency. Furthermore, Fast R-CNN introduces the Region of Interest (RoI) pooling layer [2], allowing feature extraction from region proposals of varying sizes without the need for expensive resizing operations. This enables precise alignment of features with the corresponding regions of interest, leading to improved localization accuracy. Moreover, Fast R-CNN adopts a unified network architecture, enabling end-to-end training of both the region proposal and object detection tasks. This approach ensures better optimization, and facilitates seamless integration of different components, contributing to the overall efficiency and effectiveness of the model.\n\n\n3.2.2 RoI Pooling Layer\nThe RoI pooling layer employs max pooling to transform features within any valid region of interest into a compact feature map with a fixed spatial extent of H × W (e.g., 7 × 7) [2], where H and W are layer hyper-parameters independent of any specific RoI. A more efficient training strategy leverages feature sharing throughout the process. During Fast R-CNN training, stochastic gradient descent (SGD) minibatches are hierarchically sampled: N images are initially sampled, followed by R/N RoIs from each image. Notably, both during forward and backward passes, ROIs from the same image share computation and memory. Fixed-size feature maps yielded from RoI pooling are input to fully connected layers for object classification and bounding-box regression. Feature classification aims to detect features and assign class probabilities for each proposed location, while bounding box regression seeks to enhance the localization accuracy of detected features. Fast R-CNN introduces multitasking loss functions that amalgamate classification and bounding box regression losses. This enables joint training of both tasks, ensuring the network learns to predict object location and precise bounding boxes concurrently.\n\n\n\nFlowchart of Fast R-CNN\n\n\n\n\n3.2.3 Advantages of Fast R-CNN\nIn contrast to R-CNN, which comprises multiple independent steps (such as region proposal generation, feature extraction, classification, and bounding box regression), Fast R-CNN integrates these processes within a unified network architecture. This enables end-to-end training of the entire pipeline, leading to improved optimization and potentially higher accuracy. Fast R-CNN achieves variable sharing across all spatial dimensions of an image. Unlike R-CNN, which extracts features independently for each image, Fast R-CNN performs feature extraction on the entire image only once. This shared computation across convolutional features significantly minimizes redundant calculations, expediting the inference process."
  },
  {
    "objectID": "Literature-Review.html#faster-r-cnn",
    "href": "Literature-Review.html#faster-r-cnn",
    "title": "3  Literature Review",
    "section": "3.3 Faster R-CNN",
    "text": "3.3 Faster R-CNN\nCompared to preceding methodologies, Faster R-CNN achieves more efficient object recognition by seamlessly integrating regional proposal steps directly into the network architecture. Subsequent advancements have expanded upon this framework, establishing it as a fundamental component in object recognition. The architecture commences with a feature extraction backbone, typically a convolutional neural network (CNN) pre-trained on extensive datasets such as ImageNet. This backbone extracts pertinent features from the input image.\n\n3.3.1 Region Proposal Network (RPN)\nFaster R-CNN further improves the efficiency of object detection by introducing the Region Proposal Network (RPN), which generates region proposals directly from the convolutional feature maps. It eliminates the need for external region proposal methods like Selective Search, resulting in faster and more accurate proposal generation.\nThe feature maps derived from the backbone network are input to a Region Proposal Network (RPN) [3]. The RPN, a compact fully convolutional network, employs a sliding window approach (typically 3x3) over the feature maps to generate region proposals. The RPN yields a collection of bounding box proposals accompanied by objectness scores, indicating the probability of containing an object. These proposals are generated by leveraging predefined anchor boxes of varying scales and aspect ratios.\n\n\n3.3.2 Anchor Boxes\nThe RPN generates region proposals by predicting offsets and scales for a predefined set of anchor boxes at each spatial position in the feature maps. These anchor boxes, varying in size and aspect ratio, serve as reference boxes for the proposal generation process.\n\n\n3.3.3 Region of Interest (RoI)\nRegion of Interest (RoI) refers to a specific area or region within an image that is selected for further analysis or processing. In the context of object detection and image segmentation tasks, RoIs typically represent regions where objects of interest are located.\n\n\n3.3.4 Classifier and Bounding Box Regressor\nThe RoI-pooled or RoIAlign features are separately input into branches for classification and bounding box regression.\n\nClassification: The features traverse a classifier (e.g., fully connected layers followed by softmax) to predict the probability of each region proposal belonging to different object classes. Bounding Box Regression: Another set of fully connected layers predicts refined bounding box coordinates for each region proposal.\nLoss Function: The network undergoes end-to-end training using a multi-task loss function, which combines losses from the RPN (objectness score prediction and bounding box regression) with those from the classification and bounding box regression branches.\n\n\n\n3.3.5 Non-Maximum Suppression (NMS)\nFollowing prediction, non-maximum suppression is employed to discard redundant and overlapping detections based on their confidence scores and bounding box coordinates.\n\n\n3.3.6 Output\nThe final output comprises a collection of object detections alongside their bounding boxes and corresponding class labels.\n\n\n\nFlowchart of Faster R-CNN @ren2015fasterrcnn\n\n\n\n\n3.3.7 Advantages of Faster R-CNN\n\nEfficient Region Proposal Generation: The Region Proposal Network (RPN) significantly reduces the computational load of region proposal methods, enabling nearly cost-free region proposals by sharing convolutional features.\nEnd-to-End Training: RPN and Fast R-CNN can be jointly trained to share convolutional features, leading to a unified network architecture.\nHigh-Quality Proposals: RPN produces high-quality region proposals, enhancing detection accuracy compared to traditional methods like Selective Search and EdgeBoxes.\nPractical Implementation: The proposed method achieves state-of-the-art object detection accuracy on benchmarks like PASCAL VOC [3] while maintaining a practical frame rate of 5fps on a GPU."
  },
  {
    "objectID": "Literature-Review.html#mask-r-cnn",
    "href": "Literature-Review.html#mask-r-cnn",
    "title": "3  Literature Review",
    "section": "3.4 Mask R-CNN",
    "text": "3.4 Mask R-CNN\nMask R-CNN extends Faster R-CNN by adding a branch to predict the segmentation mask at each Region of Interest (RoI), alongside existing branches for classification and bounding box regression. This is achieved by introducing a small, fully convolutional network on top of each RoI. Below we briefly describe the architecture components.\n\n3.4.1 Backbone CNN\nSimilar to Faster R-CNN, Mask R-CNN begins with a backbone that extracts features from the input image. This backbone network is typically pre-trained on a large dataset like ImageNet to learn generic image features.\n\n\n3.4.2 Region Proposal Network (RPN)\nThe RPN takes feature maps from the backbone network and generates region proposals using anchor boxes, refined based on their likelihood of containing objects. The region proposals generated by the RPN are subsequently used for both object detection, and instance segmentation in the Mask R-CNN framework.\n\n\n3.4.3 Region of interest Align (RoIAlign)\nFor each region proposal generated by the RPN, features are extracted from the feature maps using RoIAlign. RoIAlign is a technique used in CNNs for object detection tasks. RoIAlign improves upon RoIPool, a previous method, by eliminating the quantization step in the pooling operation, resulting in more accurate feature extraction from regions of interest [4]. It precisely aligns features extracted from arbitrary-shaped regions with the spatial layout of the feature map, enabling more accurate object localization and segmentation.\nThe RoIAlign layer extracts features from each region proposal, preserving spatial information better than previous pooling methods, ensuring accurate alignment of features with the region of interest.\n\n\n3.4.4 Parallel Branches\nMask R-CNN introduces parallel branches for object detection and instance segmentation.\n\nObject Detection Branch: Determines the class of each object within the region proposal and refines bounding box coordinates through classification and regression. – Mask Prediction Branch: Predicts segmentation masks for each object within the region proposal, generating pixel-level masks for object instances using a small fully convolutional network applied to each RoI.\nLoss Functions: Mask R-CNN employs a multi-function loss function that combines object detection (classification and bounding box regression) and instance segmentation (mask prediction) losses, weighted by hyperparameters.\n\n\\[\nL = L_{cls} + L_{box} + L_{mask}\n\\]\n\nTraining: The entire Mask R-CNN network is trained end-to-end using backpropagation with stochastic gradient descent (SGD) or other optimization algorithms, typically starting with pre-trained weights and fine-tuning for the specific task.\n\n\n\n\nMask R-CNN framework for Instance Segmentation @he2020maskrcnn\n\n\n\n\n3.4.5 Advantages over Faster R-CNN\n\nOffers pixel-level segmentation alongside object detection and bounding box regression.\nFacilitates instance segmentation, segmenting each object instance in an image separately.\nEnhances understanding of object shapes and boundaries with finer detail.\nDespite increased complexity, maintains comparable speed and efficiency, particularly in scenarios necessitating instance-level segmentation.\n\nOverall, Mask R-CNN represents a substantial advancement over previous methods by integrating object detection and instance segmentation into a unified architecture, rendering it a versatile solution for diverse computer vision applications.\n\n\n\n\n[1] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in 2014 IEEE conference on computer vision and pattern recognition, 2014, pp. 580–587. doi: 10.1109/CVPR.2014.81\n\n\n[2] R. Girshick, “Fast r-CNN,” in 2015 IEEE international conference on computer vision (ICCV), 2015, pp. 1440–1448. doi: 10.1109/ICCV.2015.169\n\n\n[3] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-CNN: Towards real-time object detection with region proposal networks,” in Advances in neural information processing systems, 2015, pp. 91–99.\n\n\n[4] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-CNN,” in Proceedings of the IEEE international conference on computer vision (ICCV), 2017, pp. 2961–2969. doi: 10.1109/ICCV.2017.322"
  },
  {
    "objectID": "methodology.html#fast-r-cnn-training",
    "href": "methodology.html#fast-r-cnn-training",
    "title": "4  Methodologies",
    "section": "4.1 Fast R-CNN Training",
    "text": "4.1 Fast R-CNN Training\nFast R-CNN follows a single-stage training process [1]. It begins by processing the image through convolutional and max pooling layers, resulting in a convolutional feature map. Each object proposal then undergoes region of interest pooling, generating fixed-length feature vectors. These feature vectors are fed through fully connected layers, leading to two output layers: one for object class probabilities and the other for real-valued object coordinates. Fast R-CNN’s training approach enables higher detection quality, updates all layers during training, and eliminates the need for disk storage for feature caching, contributing to its efficiency and accuracy [1]."
  },
  {
    "objectID": "methodology.html#faster-r-cnn-training",
    "href": "methodology.html#faster-r-cnn-training",
    "title": "4  Methodologies",
    "section": "4.2 Faster R-CNN Training",
    "text": "4.2 Faster R-CNN Training\nOn the other hand, Faster R-CNN training is a two-stage process, jointly training a Region Proposal Network (RPN) and a Fast R-CNN detector [2] . In the first stage, the RPN generates region proposals by sliding a small network over the convolutional feature map, refining them, and assigning scores based on their likelihood of containing objects. These proposals serve as input for the second stage, where the convolutional feature map undergoes region of interest (RoI) pooling. This process results in fixed-length feature vectors for each proposal, which are then processed through fully connected layers to predict object class probabilities and refine object bounding box coordinates [1]. This joint training approach allows Faster R-CNN to produce high-quality region proposals and accurately classify and localize objects in images.\n\n4.2.1 Region Proposal Network (RPN)\nTo produce the object proposals used during training, a Region Proposal Network (RPN) is used. The RPN takes in an image and produces a set of object proposals with an associating score. This region proposal is generated by sliding a small network over the convolutional feature map. At the center of each sliding window is a Translation-Invariant Anchor [2]. This anchor is a reference box that each proposal in that region is relative to. These anchors being translation-invariant means that regardless if the image is translated in any way the prediction of that region should not change.\n\n\n\nImproved anchor boxes in Faster R-CNN @zhou2022visual\n\n\nThree anchor boxes are depicted within each grid, as illustrated above. The image encompasses numerous points, equivalent to a 600 x 600 image, accommodating a 38 x 38 network overlay. The blue point within the image signifies the center of the grid. Each grid center is associated with three anchor boxes and three squares of varying sizes, representing the original anchor boxes delineated in the image [3].\n\n\n4.2.2 Multi-task Loss Function\nRPNs use positive and negative values to assign anchors. A positive value represents a high Intersection-over-Union overlap between the anchor and the ground truth box [2]. A negative value represents the dissociation of an anchor and a certain prediction. The higher the value, the more associated the anchor is to a certain prediction. These values are used in a multi-task loss function. This function will both train for classification and perform bounding box regression. The loss function is defined as:\n\\[\nL = L_{cls} + λ * L_{reg}\n\\]\nWhere the RPN consist of two losses.\n\\[\nL = (1/N) * Σ[L_{cls}(p_i, p_i*) + λ * L_{reg}(t_i, t_i*)]\n\\]\nThe authors describe the above equation by saying [2], “Here, i is the index of an anchor in a mini-batch and pi is the predicted probability of anchor I being an object. The ground-truth label p ∗ i is 1 if the anchor is positive, and is 0 if the anchor is negative. ti is a vector representing the 4 parameterized coordinates of the predicted bounding box, and t ∗ i is that of the ground-truth box associated with a positive anchor. The classification loss Lcls is log loss over two classes (object vs. not object). For the regression loss, we use Lreg (ti, t∗ i ) = R(ti − t ∗ i ) where R is the robust loss function (smooth L1). The term p ∗ i Lreg means the regression loss is activated only for positive anchors (p ∗ i = 1) and is disabled otherwise (p ∗ i = 0). The outputs of the cls and reg layers consist of {pi} and {ti} respectively. The two terms are normalized with Ncls and Nreg, and a balancing weight λ.”\nFor regression, the formula for the parameterized coordinated of the predicted bounding box:\n\\[\nΔx = (x' - x) / w,\\quad Δy = (y' - y) / h,\\quad Δw = log(w' / w),\\quad Δh = log(h' / h)\n\\]\n\n(x, y, w, h) represents the coordinates of the default anchor box,\n(x’, y’, w’, h’) represents the coordinates of the predicted bounding box,\nΔx, Δy, Δw, and Δh are the parameterized adjustments to the coordinates of the default anchor box to obtain the predicted bounding box coordinates.\n\n\n\n4.2.3 Optimization\nTo optimize the data, backpropagation and stochastic gradient descent are utilized. Start with the initializing from a zero-mean Gaussian distribution. Perform back-propagation through RoI pooling layers and pass partial derivatives of the loss function concerning an activation input “xi” in the RoI pooling layer.\n\\[\n{{∂L} \\over {∂x_i}} = \\Sigma_r \\Sigma_j [i = i*(r,j)] { {∂L} \\over {∂y_{rj} }}\n\\]\nThe authors describe this backward function as [1], “In words, for each mini-batch RoI r and each pooling output unit yrj, the partial derivative ∂L/∂yrj is accumulated if i is the argmax selected for yrj by max pooling. In back-propagation, the partial derivatives ∂L/∂yrj are already computed by the backward function of the layer on top of the RoI pooling layer”. As iterations continue, weights and biases will be tuned towards values that optimize predictions.\nThis formula represents the accumulation of partial derivatives with respect to the output units of the RoI pooling layer during backpropagation. It describes how the gradients of the loss function with respect to the outputs of the RoI pooling layer are computed and accumulated during the backward pass of the neural network training process. It helps us understand how the network learns from its mistakes and improves its predictions over time."
  },
  {
    "objectID": "methodology.html#metrics",
    "href": "methodology.html#metrics",
    "title": "4  Methodologies",
    "section": "4.3 Metrics",
    "text": "4.3 Metrics\nOne data metric utilized in the chosen paper, and that will also be used in the analysis, is the mean average precision (mAP). This evaluation metric is typically used for object detection, and consists of multiple sub-metrics such as the confusion matrix, recall, precision, and the intersection over union. mAP is mathematically defined as, n is the number of classes, and APk is the average precision of the current class (k). Mean average precision basically takes an established ground truth box around the target and compares it to the detected box from the deep learning model, yielding an accuracy score. A higher accuracy score implies that the deep learning model is accurate with its detection.\n\\[\nmAP = (1/n) * \\sum_{k=1}^{k=n}(AP_k)\n\\]\n\n\\(AP_k\\) = the Average Precision of class \\(k\\)\n\\(n\\) = the number of classes.\n\n\n\n\n\n[1] R. Girshick, “Fast r-CNN,” in 2015 IEEE international conference on computer vision (ICCV), 2015, pp. 1440–1448. doi: 10.1109/ICCV.2015.169\n\n\n[2] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-CNN: Towards real-time object detection with region proposal networks,” in Advances in neural information processing systems, 2015, pp. 91–99.\n\n\n[3] Y. Zhou, X. Wang, and L. Zhang, “Visual identification and pose estimation algorithm of nut tightening robot system.” Feb. 2022. doi: 10.21203/rs.3.rs-1391065/v1"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "5  Analysis",
    "section": "",
    "text": "The demonstration utilized the ResNet backbone network in conjunction with the COCO dataset. The Microsoft Common Objects in Context (COCO) dataset [1] is a large-scale dataset for object detection, segmentation, and captioning tasks. It contains over 200,000 images, each annotated with bounding boxes around objects in various categories such as people, animals, vehicles, and household items. Additionally, each image is annotated with pixel-level segmentation masks for object instances. The COCO dataset is widely used in computer vision research for benchmarking and evaluating object detection and segmentation algorithms.\nFaster R-CNN, known for its proficiency in object detection tasks, has showcased remarkable performance [2] . However, challenges persist regarding its robustness to environmental variations such as shadows, occlusions, and perspective distortions. Furthermore, mislabeling of objects remains a concern, potentially leading to incorrect identifications due to similarities with trained categories.gories.\nIn addition to Faster R-CNN, Mask R-CNN was employed, utilizing a pretrained model from the COCO dataset. This model underwent testing on both images, and a video dataset. Transitioning to YOLOv8, preparation of the data involved annotating the training set. Twenty images of tanks were manually annotated using CVAT.ai to draw bounding boxes around the targets. Subsequently, specific bounding box coordinates for YOLOv8 were obtained. Notably, all tank images for training and testing were sourced from Flickr.\nTo configure the dataset appropriately for YOLOv8, a yaml file was created, specifying the dataset’s structure. This file included details such as the overall path to the data, the locations of the training, validation, and test sets, and the names of the classes to be identified.\nFollowing data preprocessing, YOLOv8 training commenced using the tank training set. Initially, twenty epochs were employed with the medium model. However, during testing, Google Colab encountered stability issues, necessitating a switch to the small model, which resulted in smooth execution.\n\n\n\n\n[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2016, pp. 770–778.\n\n\n[2] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-CNN: Towards real-time object detection with region proposal networks,” in Advances in neural information processing systems, 2015, pp. 91–99."
  },
  {
    "objectID": "results.html#faster-r-cnn",
    "href": "results.html#faster-r-cnn",
    "title": "6  Results",
    "section": "6.1 Faster R-CNN",
    "text": "6.1 Faster R-CNN\nWe used the Faster R-CNN model from TensorFlow Hub (Inception-ResNet-V2). According to the authors [1], Inception-ResNet-V2 is a deep convolutional neural network architecture introduced as part of the Inception family.\nThe Faster R-CNN results can be seen in images 1-4. In the first image, the model was fairly accurate in identifying individual chairs and a couple of the tables.\n\n\n\nFaster R-CNN result 1\n\n\n\nIn the second image, Faster R-CNN correctly identified all beetles.\n\n\n\nFaster R-CNN result 2\n\n\n\nIn the third image the model had some issues with misclassifications of the types of phones.\n\n\n\nFaster R-CNN result 3\n\n\n\nIn the fourth image there seemed to be some confusion in the distinction between birds and animals in general.\n\n\n\nFaster R-CNN result 4"
  },
  {
    "objectID": "results.html#mask-r-cnn",
    "href": "results.html#mask-r-cnn",
    "title": "6  Results",
    "section": "6.2 Mask R-CNN",
    "text": "6.2 Mask R-CNN\nImages 5-7 are the results from the Mask R-CNN model that was trained from the COCO dataset. Based on the results this model seems to perform much better than the Faster R-CNN model.\nIn image five, Mask R-CNN very accurately detected the giraffe and two zebras.\n\n\n\nMask R-CNN result 1\n\n\n\nIn image six, the model was able to clearly detect the objects with a high mean average precision.\n\n\n\nMask R-CNN result 2\n\n\n\nImage seven comes from a video that was fed to the Mask R-CNN model. In this frame, the model did have some trouble identifying all the objects.\n\n\n\nMask R-CNN result 3"
  },
  {
    "objectID": "results.html#yolov8",
    "href": "results.html#yolov8",
    "title": "6  Results",
    "section": "6.3 YOLOv8",
    "text": "6.3 YOLOv8\nThe last model used in our analysis was YOLOv8. This model also seemed to perform better than the Faster R-CNN model. Our YOLOv8 small (YOLOv8s) model was trained on a custom dataset that consisted of tanks and was subsequently tested on test photos of tanks. Images 8-10 show the results of the YOLOv8s model. The model accurately identified all of the tanks.\n\n\n\nYOLOv8 result 1\n\n\n\n\n\nYOLOv8 result 2\n\n\n\n\n\nYOLOv8 result 3\n\n\n\n\n\n\n[1] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, “Inception-v4, inception-ResNet and the impact of residual connections on learning,” in Proceedings of the thirty-first AAAI conference on artificial intelligence (AAAI-17), 2017, pp. 4278–4284."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "7  Conclusion",
    "section": "",
    "text": "The goal of this project is to review and test the methodology behind Faster R-CNN, and then evaluate alternative models such as Mask R-CNN and YOLOv8. Our analysis indicates that both Mask R-CNN, and YOLOv8 demonstrate superior performance compared to Faster R-CNN in terms of object detection accuracy and instance precision. Specifically, YOLOv8 proves to be a suitable model for real-time object detection tasks, offering efficient and rapid detection capabilities. On the other hand, Mask R-CNN exhibits higher accuracy in detecting objects within photos and videos, particularly when precise segmentation and detailed understanding of object shapes and boundaries are required. However, it is important to note that Mask R-CNN’s performance may be contingent upon the availability of sufficient computational resources, given its more intensive processing requirements compared to YOLOv8."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] R.\nGirshick, “Fast r-CNN,” in 2015 IEEE international\nconference on computer vision (ICCV), 2015, pp. 1440–1448. doi: 10.1109/ICCV.2015.169\n\n\n[2] S.\nRen, K. He, R. Girshick, and J. Sun, “Faster r-CNN: Towards\nreal-time object detection with region proposal networks,” in\nAdvances in neural information processing systems, 2015, pp.\n91–99.\n\n\n[3] R.\nGirshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\nhierarchies for accurate object detection and semantic\nsegmentation,” in 2014 IEEE conference on computer vision and\npattern recognition, 2014, pp. 580–587. doi: 10.1109/CVPR.2014.81\n\n\n[4] K.\nHe, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-CNN,”\nin Proceedings of the IEEE international conference on computer\nvision (ICCV), 2017, pp. 2961–2969. doi: 10.1109/ICCV.2017.322\n\n\n[5] Y.\nZhou, X. Wang, and L. Zhang, “Visual identification and pose\nestimation algorithm of nut tightening robot system.” Feb. 2022.\ndoi: 10.21203/rs.3.rs-1391065/v1\n\n\n[6] K.\nHe, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition (CVPR), 2016, pp.\n770–778.\n\n\n[7] C.\nSzegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, “Inception-v4,\ninception-ResNet and the impact of residual connections on\nlearning,” in Proceedings of the thirty-first AAAI conference\non artificial intelligence (AAAI-17), 2017, pp. 4278–4284.\n\n\n[8] J.\nR. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M.\nSmeulders, “Selective search for object recognition,”\nInternational Journal of Computer Vision, vol. 104, no. 2, pp.\n154–171, 2013.\n\n\n[9] C.\nL. Zitnick and P. Dollár, “Edge boxes: Locating object proposals\nfrom edges,” in European conference on computer vision,\n2014, pp. 391–405."
  },
  {
    "objectID": "contributions.html",
    "href": "contributions.html",
    "title": "Appendix A — Contributions",
    "section": "",
    "text": "A.0.1 Hector Gavilanes – Project Lead\nPrepared the GitHub repository and pages for hosting the research paper online. Proofread and edited all chapters for the final version of the research paper. Researched and implemented code for Faster R-CNN, Mask R-CNN, and YOLOv8 methodologies. Analyzed, experimented, and made inferences during training, validation, and test phases. Developed a custom class label for detecting military tanks using YOLOv8. Evaluated the performance between various object detection and segmentation models. Kept communication with the professor, and peers.\n\n\nA.0.2 Jacob Knight – Deep Learning Architect\nWrote about the different methodologies used, such as Region Proposal Network, Translation-Invariant Anchor, and back-propagation. Conducted peer reviews of others’ work.\n\n\nA.0.3 Kyle Knuth – Data Scientist\nWrote the objectives, introduction, metrics, analysis, and results. Helped gather training and testing images for the YOLOv8 model and assisted with the annotation of the training data. Assisted in the training and implementation of the YOLOv8 model.\n\n\nA.0.4 Sai Meghana – Software Engineer\nResearched and wrote the literature review chapter, and made comparison between different methodologies. Drafted flowcharts to explain the architecture of each model. Assisted in writing methodologies chapter."
  }
]