[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer Vision Object Detection",
    "section": "",
    "text": "Preface\nComputer Vision Object Detection using Faster R-CNN, Mask R-CNN, and YOLOv8\nPlease visit our repository in GitHub.\n\n1 + 1\n\n2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Object detection is a fundamental task in computer vision, playing a crucial role in various applications such as autonomous vehicles, surveillance systems, and medical imaging. Recent advancements in deep learning have led to the creation of sophisticated architectures such as Faster R-CNN, Mask R-CNN, and YOLO (You Only Look Once), all of which have substantially improved object detection performance. These architectures have revolutionized the field by achieving superior accuracy in detecting objects within images. However, despite significant advancements, a critical trade-off between accuracy and processing speed persists. It is essential to carefully consider this balance when selecting the most suitable model for practical applications.\nThe efficiency of object detection models is essential, particularly in real-time applications where rapid decision-making is vital. Currently, object detection primarily relies on region proposal methods and region CNNs. For instance, Selective Search, a widely used region proposal method, exhibits a speed of 2 seconds per image when implemented on the CPU, which is considered slow for many real-world applications. On the other hand, EdgeBoxes, another region proposal method, offers improved speed, clocking in at 0.2 seconds per image. However, a notable constraint remains— even with advancements, the region proposal step still demands substantial computational resources when executed on the CPU rather than the GPU.\nMoreover, comparing the performance of Faster R-CNN with region proposal methods poses challenges. Since R-CNNs leverage GPU acceleration while region proposal methods primarily rely on CPU computation. A fair comparison between the two becomes challenging. To address this issue, Region Proposal Networks (RPNs) were introduced in 2015 as part of the Faster R-CNN framework [1]. A novel approach that integrates the region proposal step directly into the object detection algorithm itself. This architectural innovation aims to streamline the object detection process and improve overall efficiency by leveraging GPU resources effectively.\n\n\n\n\n[1] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-CNN: Towards real-time object detection with region proposal networks,” in Advances in neural information processing systems, 2015."
  },
  {
    "objectID": "summary.html#subtitle",
    "href": "summary.html#subtitle",
    "title": "2  Summary",
    "section": "2.1 Subtitle",
    "text": "2.1 Subtitle\ntext\n\n2.1.1 Subtitle 2\nThis is last level.\nMatrices\n\\[\n\\left[\\begin{matrix}1 & 0\\\\0 & 1\\\\2 & 3\\end{matrix}\\right]\n\\]\nAnother example.\n\\[\n\\sqrt{x^2+1}\n\\]\nThis is a simple math expression without numbering separated from text."
  },
  {
    "objectID": "Literature-Review.html#r-cnn-region-based-convolutional-network",
    "href": "Literature-Review.html#r-cnn-region-based-convolutional-network",
    "title": "3  Literature Review",
    "section": "3.1 R-CNN (Region-Based Convolutional Network)",
    "text": "3.1 R-CNN (Region-Based Convolutional Network)\nR-CNN was a breakthrough in object detection. It employed a multi-stage approach that involved a selective search for generating region proposals followed by a convolutional neural network for feature extraction and a support vector machine (SVM) for object classification within each region.\n\n3.1.1 Region Proposal Generation\nR-CNN began with generating region proposals using the selective search algorithm [1]. Selective search is a method for identifying potential object regions in an image based on low-level features such as color, texture, and intensity. It produces a set of bounding boxes that mostly have objects.\n\n\n3.1.2 Feature Extraction\nFollowing the generation of region proposals, R-CNN employed a pre-trained convolutional neural network to independently extract features from each region. Typically, the chosen CNN model was AlexNet, pre-trained on the ImageNet dataset specifically for image classification tasks.\n\n\n3.1.3 Fine-tuning and Classification\nFollowing feature extraction, the extracted features were input into a distinct classifier to ascertain the presence of objects within the regions. R-CNN utilized a support vector machine (SVM) for this classification task. Each SVM was trained to discern whether the feature corresponded to a particular object or background.\n\n\n3.1.4 Bounding Box Regression\nFollowing classification, R-CNN conducted bounding box regression to enhance the accuracy of the detected object locations. This process adjusts the bounding boxes produced by the region proposal algorithm to align more precisely with the actual object locations within the regions.\n\n\n3.1.5 Non-Maximum Suppression\nLastly, R-CNN implemented non-maximum suppression to eliminate redundant detections, ensuring that each object is detected only once.\n\n\n\nFigure 1: Flowchart of R-CNN\n\n\n\n\n3.1.6 Drawbacks\nAn initial drawback of the R-CNN architecture was its computational inefficiency during inference, primarily stemming from its sequential processing of region proposals. Processing each region proposal independently led to redundant computations and prolonged inference times."
  },
  {
    "objectID": "Literature-Review.html#fast-r-cnn",
    "href": "Literature-Review.html#fast-r-cnn",
    "title": "3  Literature Review",
    "section": "3.2 Fast R-CNN",
    "text": "3.2 Fast R-CNN\nFast R-CNN represents a faster and more efficient iteration of the original R-CNN for object detection. It tackles the computational inefficiencies of R-CNN by introducing a unified architecture that consolidates region proposal generation, feature extraction, and object classification into a single network. This approach dramatically reduces redundant computations and accelerates the inference process.\n\n3.2.1 Architecture\nFast R-CNN employs a regional pattern network (RPN) to directly generate region proposals from the convolutional feature maps of the input image [2]. This eliminates the necessity for external region proposals, such as selective search, as used in the original R-CNN. The RPN functions by sliding a small grid (essentially a compact CNN) across the evolving feature map to predict the spatial dimensions (bounding boxes) and associated probability scores for objects within each sliding window. Subsequently, these local parameters serve as inputs for the subsequent stage of the process.\n\n\n3.2.2 Feature Extraction with RoI Pooling\nThe RoI pooling layer employs max pooling to transform features within any valid region of interest into a compact feature map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters independent of any specific RoI. A more efficient training strategy leverages feature sharing throughout the process. During Fast R-CNN training, stochastic gradient descent (SGD) minibatches are hierarchically sampled: N images are initially sampled, followed by R/N RoIs from each image. Notably, both during forward and backward passes, ROIs from the same image share computation and memory. Fixed-size feature maps yielded from RoI pooling are input to fully connected layers for object classification and bounding-box regression. Feature classification aims to detect features and assign class probabilities for each proposed location, while bounding box regression seeks to enhance the localization accuracy of detected features. Fast R-CNN introduces multitasking loss functions that amalgamate classification and bounding box regression losses. This enables joint training of both tasks, ensuring the network learns to predict object location and precise bounding boxes concurrently.\n\n\n\nFigure 2: Flowchart of Fast R-CNN\n\n\n\n\n3.2.3 Advantages of Fast R-CNN\nIn contrast to R-CNN, which comprises multiple independent steps (such as region proposal generation, feature extraction, classification, and bounding box regression), Fast R-CNN integrates these processes within a unified network architecture. This enables end-to-end training of the entire pipeline, leading to improved optimization and potentially higher accuracy. Fast R-CNN achieves variable sharing across all spatial dimensions of an image. Unlike R-CNN, which extracts features independently for each image, Fast R-CNN performs feature extraction on the entire image only once. This shared computation across convolutional features significantly minimizes redundant calculations, expediting the inference process."
  },
  {
    "objectID": "Literature-Review.html#faster-r-cnn",
    "href": "Literature-Review.html#faster-r-cnn",
    "title": "3  Literature Review",
    "section": "3.3 Faster R-CNN",
    "text": "3.3 Faster R-CNN\nCompared to preceding methodologies, Faster R-CNN achieves swifter and more efficient object recognition by seamlessly integrating regional proposal steps directly into the network architecture. Subsequent advancements have expanded upon this framework, establishing it as a fundamental component in object recognition. The architecture commences with a feature extraction backbone, typically a convolutional neural network (CNN) pre-trained on extensive datasets such as ImageNet. This backbone extracts pertinent features from the input image.\n\n3.3.1 Region Proposal network (RPN)\nThe feature maps derived from the backbone network are input to a Region Proposal Network (RPN) [3]. The RPN, a compact fully convolutional network, employs a sliding window approach (typically 3x3) over the feature maps to generate region proposals. The RPN yields a collection of bounding box proposals accompanied by objectness scores, indicating the probability of containing an object. These proposals are generated by leveraging predefined anchor boxes of varying scales and aspect ratios.\n\n\n3.3.2 Anchor Boxes\nThe RPN generates region proposals by predicting offsets and scales for a predefined set of anchor boxes at each spatial position in the feature maps. These anchor boxes, varying in size and aspect ratio, serve as reference boxes for the proposal generation process.\n\n\n3.3.3 Region of Interest (RoI) Pooling or RoIAlign\nFor each region proposal generated by the RPN, features are extracted from the feature maps using RoI pooling or RoIAlign. RoI pooling or RoIAlign ensures that features from the region proposals are aligned and resized to a fixed size, facilitating further processing.\n\n\n3.3.4 Classifier and Bounding Box Regressor\nThe RoI-pooled or RoIAlign features are separately input into branches for classification and bounding box regression.\n\nClassification: The features traverse a classifier (e.g., fully connected layers followed by softmax) to predict the probability of each region proposal belonging to different object classes. Bounding Box Regression: Another set of fully connected layers predicts refined bounding box coordinates for each region proposal.\nLoss Function: The network undergoes end-to-end training using a multi-task loss function, which combines losses from the RPN (objectness score prediction and bounding box regression) with those from the classification and bounding box regression branches.\n\n\n\n3.3.5 Non-Maximum Suppression (NMS)\nFollowing prediction, non-maximum suppression is employed to discard redundant and overlapping detections based on their confidence scores and bounding box coordinates.\n\n\n3.3.6 Output\nThe final output comprises a collection of object detections alongside their bounding boxes and corresponding class labels.\n\n\n\nFigure 3: Flowchart of Faster R-CNN [3]\n\n\n\n\n3.3.7 Advantages of Faster R-CNN\n\nEfficient Region Proposal Generation: The Region Proposal Network (RPN) significantly reduces the computational load of region proposal methods, enabling nearly cost-free region proposals by sharing convolutional features.\nEnd-to-End Training: RPN and Fast R-CNN can be jointly trained to share convolutional features, leading to a unified network architecture.\nHigh-Quality Proposals: RPN produces high-quality region proposals, enhancing detection accuracy compared to traditional methods like Selective Search and EdgeBoxes.\nPractical Implementation: The proposed method achieves state-of-the-art object detection accuracy on benchmarks like PASCAL VOC while maintaining a practical frame rate of 5fps on a GPU."
  },
  {
    "objectID": "Literature-Review.html#mask-r-cnn",
    "href": "Literature-Review.html#mask-r-cnn",
    "title": "3  Literature Review",
    "section": "3.4 Mask R-CNN",
    "text": "3.4 Mask R-CNN\nMask R-CNN extends Faster R-CNN by adding a branch to predict the segmentation mask at each Region of Interest (RoI), alongside existing branches for classification and bounding box regression. This is achieved by introducing a small, fully convolutional network on top of each RoI.\n\n3.4.1 Architecture\n\n\n3.4.2 Backbone CNN\nSimilar to Faster R-CNN, Mask R-CNN begins with a backbone Convolutional Neural Network (CNN) that extracts features from the input image. This backbone network is typically pre-trained on a large dataset like ImageNet to learn generic image features.\n\n\n3.4.3 Regional Proposal Network (RPN)\nThe RPN takes feature maps from the backbone network and generates region proposals using anchor boxes, refined based on their likelihood of containing objects.\n\n\n3.4.4 RoI Align\nThe RoI align layer extracts features from each region proposal, preserving spatial information better than previous pooling methods, ensuring accurate alignment of features with the region of interest.\n\n\n3.4.5 Parallel Branches\nMask R-CNN introduces parallel branches for object detection and instance segmentation.\n\nObject Detection Branch: Determines the class of each object within the region proposal and refines bounding box coordinates through classification and regression. – Mask Prediction Branch: Predicts segmentation masks for each object within the region proposal, generating pixel-level masks for object instances using a small fully convolutional network applied to each RoI.\nLoss Functions: Mask R-CNN employs a multi-function loss function that combines object detection (classification and bounding box regression) and instance segmentation (mask prediction) losses, weighted by hyperparameters.\n\n\\[\nL = L_{cls} + L_{box} + L_{mask}\n\\]\n\nTraining: The entire Mask R-CNN network is trained end-to-end using backpropagation with stochastic gradient descent (SGD) or other optimization algorithms, typically starting with pre-trained weights and fine-tuning for the specific task.\n\n\n\n\nFigure 4: Mask R-CNN framework for Instance Segmentation [4]\n\n\n\n\n3.4.6 Advantages over Faster R-CNN\n\nOffers pixel-level segmentation alongside object detection and bounding box regression.\nFacilitates instance segmentation, segmenting each object instance in an image separately.\nEnhances understanding of object shapes and boundaries with finer detail.\nDespite increased complexity, maintains comparable speed and efficiency, particularly in scenarios necessitating instance-level segmentation.\n\nOverall, Mask R-CNN represents a substantial advancement over previous methods by integrating object detection and instance segmentation into a unified architecture, rendering it a versatile solution for diverse computer vision applications.\n\n\n\n\n[1] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in 2014 IEEE conference on computer vision and pattern recognition, 2014, pp. 580–587. doi: 10.1109/CVPR.2014.81\n\n\n[2] R. Girshick, “Fast r-CNN,” in 2015 IEEE international conference on computer vision (ICCV), 2015, pp. 1440–1448. doi: 10.1109/ICCV.2015.169\n\n\n[3] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-CNN: Towards real-time object detection with region proposal networks,” in Advances in neural information processing systems, 2015.\n\n\n[4] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-CNN,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 2, pp. 386–397, Feb. 2020, doi: 10.1109/TPAMI.2018.2844175"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] K.\nHe, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-CNN,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 42, no. 2, pp. 386–397, Feb. 2020, doi: 10.1109/TPAMI.2018.2844175\n\n\n[2] R.\nGirshick, “Fast r-CNN,” in 2015 IEEE international\nconference on computer vision (ICCV), 2015, pp. 1440–1448. doi: 10.1109/ICCV.2015.169\n\n\n[3] S.\nRen, K. He, R. Girshick, and J. Sun, “Faster r-CNN: Towards\nreal-time object detection with region proposal networks,” in\nAdvances in neural information processing systems, 2015.\n\n\n[4] R.\nGirshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\nhierarchies for accurate object detection and semantic\nsegmentation,” in 2014 IEEE conference on computer vision and\npattern recognition, 2014, pp. 580–587. doi: 10.1109/CVPR.2014.81"
  }
]