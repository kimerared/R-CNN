---
jupyter: python3
---

# Methodologies

## Faster R-CNN Training

Faster R-CNN training is a two-stage process, jointly training a Region Proposal Network (RPN) and a Fast R-CNN detector @ren2015fasterrcnn . In the first stage, the RPN generates region proposals by sliding a small network over the convolutional feature map, refining them, and assigning scores based on their likelihood of containing objects. These proposals serve as input for the second stage, where the convolutional feature map undergoes region of interest (RoI) pooling. This process results in fixed-length feature vectors for each proposal, which are then processed through fully connected layers to predict object class probabilities and refine object bounding box coordinates [@ren2015fasterrcnn, @girshick2015fastrcnn]. This joint training approach allows Faster R-CNN to produce high-quality region proposals and accurately classify and localize objects in images.

### Region Proposal Network (RPN)

To produce the object proposals used during training, a Region Proposal Network (RPN) is used. The RPN takes in an image and produces a set of object proposals with an associating score. This region proposal is generated by sliding a small network over the convolutional feature map. At the center of each sliding window is a Translation-Invariant Anchor @ren2015fasterrcnn. This anchor is a reference box that each proposal in that region is relative to. These anchors being translation-invariant means that regardless if the image is translated in any way the prediction of that region should not change.

![Improved anchor boxes in Faster R-CNN \@zhou2022visual](images/methods/Faster-RCNN-anchor-boxes.png){style="text-align: center;" fig-alt="Improved anchor boxes in Faster R-CNN" fig-align="center"}

Three anchor boxes are depicted within each grid, as illustrated above. The image encompasses numerous points, equivalent to a 600 x 600 image, accommodating a 38 x 38 network overlay. The blue point within the image signifies the center of the grid. Each grid center is associated with three anchor boxes and three squares of varying sizes, representing the original anchor boxes delineated in the image @zhou2022visual.

### Multi-task Loss Function

RPNs use positive and negative values to assign anchors. A positive value represents a high Intersection-over-Union overlap between the anchor and the ground truth box @ren2015fasterrcnn. A negative value represents the dissociation of an anchor and a certain prediction. The higher the value, the more associated the anchor is to a certain prediction. These values are used in a multi-task loss function. This function will both train for classification and perform bounding box regression. The loss function is defined as:

$$
L = L_{cls} + λ * L_{reg}
$$

Where the RPN consist of two losses.

$$
L = (1/N) * Σ[L_{cls}(p_i, p_i*) + λ * L_{reg}(t_i, t_i*)]
$$

The authors describe the above equation by saying @ren2015fasterrcnn, "Here, i is the index of an anchor in a mini-batch and pi is the predicted probability of anchor I being an object. The ground-truth label p ∗ i is 1 if the anchor is positive, and is 0 if the anchor is negative. ti is a vector representing the 4 parameterized coordinates of the predicted bounding box, and t ∗ i is that of the ground-truth box associated with a positive anchor. The classification loss Lcls is log loss over two classes (object vs. not object). For the regression loss, we use Lreg (ti, t∗ i ) = R(ti − t ∗ i ) where R is the robust loss function (smooth L1). The term p ∗ i Lreg means the regression loss is activated only for positive anchors (p ∗ i = 1) and is disabled otherwise (p ∗ i = 0). The outputs of the cls and reg layers consist of {pi} and {ti} respectively. The two terms are normalized with Ncls and Nreg, and a balancing weight λ."

For regression, the formula for the parameterized coordinated of the predicted bounding box:

$$
Δx = (x' - x) / w,\quad Δy = (y' - y) / h,\quad Δw = log(w' / w),\quad Δh = log(h' / h)
$$

-   (x, y, w, h) represents the coordinates of the default anchor box,

-   (x', y', w', h') represents the coordinates of the predicted bounding box,

-   Δx, Δy, Δw, and Δh are the parameterized adjustments to the coordinates of the default anchor box to obtain the predicted bounding box coordinates.

### Optimization

To optimize the data, backpropagation and stochastic gradient descent are utilized. Start with the initializing from a zero-mean Gaussian distribution. Perform back-propagation through RoI pooling layers and pass partial derivatives of the loss function concerning an activation input "xi" in the RoI pooling layer.

$$
{{∂L} \over {∂x_i}} = \Sigma_r \Sigma_j [i = i*(r,j)] { {∂L} \over {∂y_{rj} }}
$$

The authors describe this backward function as @girshick2015fastrcnn, "In words, for each mini-batch RoI r and each pooling output unit yrj, the partial derivative ∂L/∂yrj is accumulated if i is the argmax selected for yrj by max pooling. In back-propagation, the partial derivatives ∂L/∂yrj are already computed by the backward function of the layer on top of the RoI pooling layer". As iterations continue, weights and biases will be tuned towards values that optimize predictions.

This formula represents the accumulation of partial derivatives with respect to the output units of the RoI pooling layer during backpropagation. It describes how the gradients of the loss function with respect to the outputs of the RoI pooling layer are computed and accumulated during the backward pass of the neural network training process. It helps us understand how the network learns from its mistakes and improves its predictions over time.

## Mask R-CNN

Mask R-CNN is a framework for object instance segmentation that efficiently detects objects in an image while generating high-quality segmentation masks for each instance. This method extends Faster R-CNN by incorporating a branch for predicting object masks alongside the existing branch for bounding box recognition @he2017maskrcnn.

### Two-Stage Process 

Mask R-CNN utilizes a two-stage target detection method. The first stage involves obtaining proposals through the Region Proposal Network (RPN), while the second stage processes features of target locations based on the proposals and regions of interest generated in the first stage, performing classification, localization, and mask prediction @fang2023improved.

Feature Extraction Network: The feature extraction network of Mask R-CNN consists of bottom-up and top-down paths. The bottom-up path, composed of residual structures of different sizes like ResNet-101, is responsible for image feature extraction @fang2023improved.

### Loss Function 

The loss function in Mask R-CNN includes components for class label, bounding box, and mask. Lclass is a classification loss, while Lmask uses binary cross-entropy loss for mask prediction @he2017maskrcnn.

The components are as follow:

-   $L_{class}$ = Classification loss to determine if an object exists.

-   $L_{mask}$ = Binary cross-entropy loss used for mask prediction.

### RoI Align

RoI Align, short for Region of Interest Align, is a technique used in object detection and segmentation tasks to extract a small feature map from each Region of Interest (RoI) with improved accuracy compared to RoI Pooling. RoI Align overcomes the quantization issues of RoI Pooling by employing bilinear interpolation to compute precise values at four regularly sampled locations within each RoI bin, ensuring better alignment with the input features @he2017maskrcnn, @erdem2020understanding.

#### Bilinear Interpolation

RoI Align utilizes bilinear interpolation to calculate exact feature values at four regularly sampled locations within each RoI bin, enhancing the alignment of extracted features with the input data @erdem2020understanding.

#### Quantization Improvement

Unlike RoI Pooling, which suffers from quantization errors, RoI Align avoids quantization issues by using bilinear interpolation to maintain accurate spatial information during feature extraction @erdem2020understanding.

The RoI Align operation involves computing the exact values of input features at four regularly sampled locations within each RoI bin using bilinear interpolation @erdem2020understanding. This precise alignment enhances the accuracy of feature extraction for each region proposal

## YOLO (You Only Look Once)

YOLOv8 employs a cross-stage partial bottleneck with two convolutions, which is called C2f module. @terven2023comprehensive Using YOLOv7’s ELAN structure, YOLOv8 uses one standard convolutional layer and maximizes the Bottleneck module to enhance the gradient branch. @wang2023bl This keeps YOLOv8 lightweight while also capturing more gradient flow information.

YOLOv8 incorporates an anchor-free model @wang2023bl with a decoupled head. This allows for independent processing of objectness, classification, and regression tasks. The anchor-free model @terven2023comprehensive design utilizes multiple branches, each of which focus on a single task. This is vital in improving the accuracy of the model.

YOLOv8 uses a sigmoid activation function as well as a softmax function for class probabilities. The loss functions used for bounding box loss and binary cross-entropy are the Complete IoU Loss, and the Distribution Focal Loss (DFL). Complete IoU uses Distance-IoU while taking aspect ratios into consideration @terven2023comprehensive. Firstly, DIoU is IoU when trying to minimize the normalized distance between central points of two bounding boxes @zheng2020distance.

When taking aspect ratios into account, the CIoU loss funtion is defined as:

$$
CIoU_{loss} = 1 - IoU + {\rho^2 (b, b^{gt}) \over c^2} + av
$$

Alpha here is defined as:

$$
\alpha = { v \over (1 - IoU) + v}
$$

The optimization function for CIoU @zheng2020distance is the following partial derivatives:

$$
CIoU_{loss} = IoU_{loss} - v \cdot \left( \frac{d(x, y)}{c} \right) - \arctan \left( \frac{w_a}{h_a} - \frac{w_b}{h_b} \right) + v \cdot \left( \frac{C}{c^2} \right)
$$

$$
\frac{\partial CIoU_{loss}}{\partial x} = \frac{\partial IoU_{loss}}{\partial x} - v \cdot \frac{\partial}{\partial x} \left( \frac{d(x, y)}{c} \right) - \arctan \left( \frac{w_a}{h_a} - \frac{w_b}{h_b} \right) + v \cdot \frac{\partial}{\partial x} \left( \frac{C}{c^2} \right)
$$

Distribution Focal Loss originates from Focal Loss @li2021generalized, which was initially developed for one-stage object detection tasks. These tasks commonly encounter a significant class imbalance between foreground and background classes throughout the training process.

The Focal Loss function is defined as:

$$
FL(p_t) = - (1 - p_t)^\gamma \cdot \log(p_t)
$$

$$
p_t = \left\{ \begin{array}{ll}p & \text{when } y = 1 \\1 - p & \text{when } y = 0\end{array} \right.
$$

Li @li2021generalized describes Distribution Focal Loss (DFL) as FL that “forces the network to rapidly focus on the values near label y, by explicitly enlarging the probabilities of yi and yi+1 (nearest two to y, yi ≤ y ≤ yi+1).”

The DFL function between two stages is define below:

$$
\text{DFL}(S_i, S_{i+1}) = -(y_{i+1}) \log(S_i) + (y - y_i) \log(S_{i+1})
$$

## Metrics

\[One data metric utilized in the chosen paper, and that will also be used in the analysis, is the mean average precision (mAP). This evaluation metric is typically used for object detection, and consists of multiple sub-metrics such as the confusion matrix, recall, precision, and the intersection over union. mAP is mathematically defined as, n is the number of classes, and APk is the average precision of the current class (k). Mean average precision basically takes an established ground truth box around the target and compares it to the detected box from the deep learning model, yielding an accuracy score. A higher accuracy score implies that the deep learning model is accurate with its detection.

$$
mAP = (1/n) * \sum_{k=1}^{k=n}(AP_k)
$$

-   $AP_k$ = the Average Precision of class $k$

-   $n$ = the number of classes.
